---
title: "Modelling the individual time requirement for sport activities using previous GPS-Tracks"
author: "Anderegg Dionis, Tophinke Alissa"
date: " 04/07/2021"
output: html_document
runtime: shiny
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, include=FALSE}
library(rgdal) 
library(sf)
library(tidyverse)
library(lubridate)
library(zoo)
library(purrr)
library(base)
library(ggpubr)

if(!requireNamespace("remotes")) {
    install.packages("remotes")
}
remotes::install_github("grimbough/FITfileR")

library(FITfileR)
library(gdata)
library(plotly)
library(ggside)

#Packages for DEM import and handling
library(raster)
library(tiff)

library(MuMIn)

```

## Abstract

In this semester project, sports tracks are analysed using GPS data from "Garmin" devices to identify specific behavior patterns in jogging activities. Furthermore, the time required for jogging activities is modeled based on the individual fitness level, so that a user-specific forecast of the time required for activities is possible. Model inputs are the distance and altitude differences (upwards and downwards).

# Table of contents

1.  [Introduction](#introduction)

    1.1 [Research questions](#subparagraph1)

2.  [Methods](#methods)

    2.1 [Data sources and import](#subparagraph2)

    2.2 [Evaluation of data](#subparagraph3)

    2.3 [Model individual time requirement](#subparagraph4)

3.  [Results](#paragraph2)

    3.1 [Research Question 1](#subparagraph5)

    3.2 [Research Question 2](#subparagraph6)

    3.3 [Research Question 3](#subparagraph7)

4.  [Disussion](#paragraph3)

    4.1 [Discussionpoint1](#subparagraph8)

    4.2 [Further studies](#subparagraph9)

5.  [Appendix](#paragraph4)

## 1 Introduction <a name="introduction"></a> {#introduction}

Sportwatches are a popular tool to monitor and regulate training activities among both professional and amateur athletes. In addition to GPS data, sport watches nowadays also record altitude (e.g. based on air pressure),heart rate or further attributes. Through this collected data, own behavior patterns can be predicted, such as fitness progress, stress, etc. In this work, it will be evaluated whether existing tracks of an athlete can be used to predict the individual time for a given route. This service is already available on different online-platforms such as *googlemaps* or *SchweizMobil*, but neither the fitness level, age, weight, nor the gender are taken into account for those calculations. This is the reason why apps that can evaluate individual data bring an advantage here. However, to enable this interpretation step from own collected data, there is a need of a) evaluating the accuracy of the measured data and b) determining the variables that lead to the desired output (in this case a time estimation). In the study by @gilgen-ammann2020a, the accuracy of the recorded distances of eight commercially available sports watches from Apple, Coros, Garmin, Polar and Suunto was examined in different areas and at different speeds. The results showed that the recorded systematic errors (±limits of agreements) when all measurement areas combined ranged between 3.7 (±195.6) m and -101.0 (±231.3) m, and the mean absolute percentage errors ranged from 3.2% to 6.1%. These results lead to the conclusion that the acquired coordinates should be aligned with a suitable digital elevation model DEM (e.g. SwissALTI3D) before further use of the data.

### 1.1 Research Questions <a name="subparagraph1"></a>

The present study focuses on the following research questions:

1.  What are the most relevant influencing variables on the athlete's time requirement and speed in the analyzed data?

2.  What behavior patterns (e.g. pausing, moving) can be identified from the data? Are these behavior patterns related to distance and altitude differences? Can patterns in behavior be identified that are harmful to health? (Comparison of heart rate curve with recommended maximum heart rate).

3.  How reliably can the time required for sporting activities be predicted based on a user's individual fitness level? A model is created from past GPS tracks, which is validated using a self-experiment (determine new route using *SchweizMobil*, model time requirement and check the model output in a self-experiment).

## 2 Methods <a name="methods"></a> {#methods}

In the following subchapters the methodology is described in detail. The data sources and its import in an R-environment, the data processing and evaluation as well as the method for the creation of a time model are shown.

Since this report contains a Shiny app, the HTML will only run if the Markdown is open in the background. When opening the HTML report only the most relevant outputs are shown in order to confine to the essential results. When using the provided R-Markdown (.Rmd), the underlying R code can be followed in the corresponding method and result chapters.

### 2.1 Data sources and import <a name="subparagraph2"></a>

The data of sport tracks which can be obtained via the Garmin portal comes within the *.fit* format. The raw *.fit* file include spatio-temporal information, such as timestamp, position (Latitude and Longitude) as well as other variables like heart-rates, distance, enhanced-speed, enhanced-altitude (air pressure based) and cadence (cadence is defined as the total number of steps per minute).

A dataset (data.frame) was generated using sports tracks data from  garmin-sportwatches from two different anonymous athletes (athlete 1, athlete 2). For this purpose, the files of every athlete must be labeled with *\_a1* or *\_a2* right before the file extension *.fit* and saved in the project directory in advance. Thus, files from athlete 1 always are labeled with *\_a1.fit* at the end of the file name. An example of a track by athlete 1 would be: *"6128842359_ACTIVITY_a1.fit".* Tracks of athlete 2 are for example labeled like *6128842359_ACTIVITY_a2.fit".* This allows adding additional data from other athletes anytime. 

A for loop detects all files from athlete 1 and athlete 2 in the project directory. All files per athlete will be combined into one dataset (data.frame) and labeled with a unique athlete ID. For the further steps, the user must specify which athlete is to be considered. This choice can be made at the request  *"Pick Athlete"* in the R-Code*.* line 141. 
The following sections of methodology and results refer to the evaluation of athlete 1's tracks. The discussion also identifies differences with athlete 2 and assesses whether the methodology presented here could be suitable for evaluating other individuals.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
############### Filter data fro one athlete only 
# Files of every athlete must be labeled with _a1, _a2, a_3 or a_x before the .fit extension. (x for any further athlete)

#  Get all the files per athlete
myfiles_a1 <- list.files(".",pattern = "*a1.fit")
myfiles_a2 <- list.files(".",pattern = "*a2.fit")

# Import files from athlete 1 temporary
for (i in 1:length(myfiles_a1)) {
  varName <- paste0("temp_a1", i, ".fit")
  assign(varName, as.data.frame(records(readFitFile(myfiles_a1[i]))))
}

# Bind row for athlete 1 temporary, label as athlete 1
a1 <- mget(ls(pattern="temp_a1")) %>%
              bind_rows() %>%
  mutate(athlete = 1)

# Import files from athlete 2 temporary
for (i in 1:length(myfiles_a2)) {
  varName <- paste0("temp_a2", i, ".fit")
  assign(varName, as.data.frame(records(readFitFile(myfiles_a2[i]))))
}

# Bind rows from athlete 2 temporary, label as athelte 2
a2 <- mget(ls(pattern="temp_a2")) %>%
              bind_rows() %>%
  mutate(athlete = 2)

# Bind files from all athletes using rowbind.
full <- bind_rows(a1, a2)

keep(full, sure = TRUE)


# Filter the whole data according to athlete to be analyzed: Pick an athlete!
Pick_Athlete <- 1

full <- full %>%
  filter(athlete == Pick_Athlete)

############# Filter here! User defines which athlete is considered (andd)

#check full
head(full)
```

### 2.2 Preparation and evaluation of data <a name="subparagraph3"></a>

In order to distinguish between the different tracks run by an athlete, an individual ID per track had to be created first. This was fulfilled by determining the time lag first, i.e. the time that elapsed between 2 recorded points. A new "activity_ID" was then created if timelag \> 1 hour (3600s).

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Calculate timelag
full$timelag <- as.integer(difftime(full$timestamp, lag(full$timestamp)))
# activity_ID (new ID if timelag > 1 hour)

full$activity_conter <- ifelse(abs(full$timelag) > 3600, TRUE, FALSE) 
full$activity_conter[1] <- TRUE  # ID 1 = TRUE, as this is activity one

full$activity_ID <- cumsum(full$activity_conter == TRUE)  # create acitiviy_ID based on counter
```

Since time steps between individual activities are not relevant, the timelag is subsequently overwritten by only calculating timelags within a group with the same activity_ID. This excludes times between activities as these values are set to NA (not available).

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
full <- full %>%
  dplyr::group_by(activity_ID) %>%
  mutate(timelag = as.integer(difftime(timestamp, lag(timestamp))))
summary(full$timelag)
```

Some variables like distance, speed ("enhanced_speed") and altitude ("enhanced_altitude") are already available in the .fit files, they are compared with the self calculated results. The methodology used for this is described in more detail in the following sub-chapters.

#### 2.2.1 Compare Elevation by Pressure with DEM

As the GPS data arrived from the sportwatch comes in the wrong coordinate system, an sf file was created ("full_sf") and convert into the Swiss Swiss national coordinate system *CH1903+ LV95*.

To evaluate the altitude data of the device derived from air pressure, a suitable digital elevation model (DEM) was taken and used in order to compare the information of the *.fit* files. The used DEM is available as Cloud Optimized GeoTIFF from *Swisstopo* and is called *SwissALTI3D*. A low raster size of 2m was used to achieve high quality information. The DEM is divided into different GeoTIFF files, each covering an area of one square kilometer. Reading and merging all DEM Files from Switzerland or the Canton of St.Gallen (Since athlete 1 has only run in the canton of St.Gallen) would be very time and resource consuming. Therefore, the area of investigation was defined automatically on the basis of the tracks read in and then only the necessary DEM raster files were imported.

The URLs to the files consist of a fixed part and a part dependent on the location. This dependent part was determined on the basis of the examination area. For this purpose, each x- and y-coordinate was floor rounded to one kilometer. This corresponds to the dependent part of the URLs. Then, all unique combinations of rounded x- and y-coordinates were then listed and the necessary URLs defined by the unique combinations and the fixed part of the links. As the URLs contain the date of the latest reevaluation of the DEM, some links contain the number 2019 while some others contain the number 2020. Thus, all the links were created twice (each for 2019 and 2020). In comparison to the complete list of all available DEM files in Switzerland, the correct URLs (containing the year 2019 or 2020) were selected and used for the import of the DEM rasters. For this import, a for loop was programmed, which downloads the required files from the URLs described before and merges them into one single raster.


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Create sf and convert to CH1903+ LV95
full_sf <- st_as_sf(full, coords = c("position_long", "position_lat"),
         crs = 4326)
full_sf <- st_transform(full_sf, crs = 2056)

options(digits = 3)

# get x and y coordinates from sf geometry
full_sf$x <- st_coordinates(full_sf$geometry)[,1]
full_sf$y <- st_coordinates(full_sf$geometry)[,2]

##  digital elevation model to verify altitude for each GPS location
#    we only import the necesary data now!

# Create a Dataframe containing all positions (floor-rounded to 1km) as information source for required DHM-rasters
DHM_rasters <- data.frame(
  x = floor(full_sf$x / 1000),
  y = floor(full_sf$y / 1000)
)
# There are NA's when GPS-Position wasn't found (Activity_ID = 19) => Exclude them!
DHM_rasters <- filter(DHM_rasters, x > 0)

# Only keep the unique combinations for rounded x and y
DHM_rasters <- unique(DHM_rasters[c("x", "y")])

# Create the Download-Links of the tiff-files from Swisstopo: 
#  Sometimes data is from 2020, sometimes from 2019. Links aren't the same. So every combination has to be produced for 2019 data and 2020 data.
DHM_URL <- c(
  paste("https://data.geo.admin.ch/ch.swisstopo.swissalti3d/swissalti3d_2019_", DHM_rasters$x, 
        "-", DHM_rasters$y, "/swissalti3d_2019_", DHM_rasters$x, "-", DHM_rasters$y, 
        "_2_2056_5728.tif", sep = ""),
  paste("https://data.geo.admin.ch/ch.swisstopo.swissalti3d/swissalti3d_2020_", DHM_rasters$x, 
        "-", DHM_rasters$y, "/swissalti3d_2020_", DHM_rasters$x, "-", DHM_rasters$y,
        "_2_2056_5728.tif", sep = ""))

# Create a data.frame for join later...
DHM_URL <- as.data.frame(DHM_URL)

# Read all available DEM-sources for whole Switzerland
DHM_full <- read_csv("DHM_CH.csv", col_names = "DHM_URL")

# Keep only DEM-sources, which are available in the source DHM_full (wohle Switzerland)
#  This ensures that the correct link (2019 or 2020) is chosen from DHM_full
DHM_required <- inner_join(DHM_URL, DHM_full)

# Now import all the required URL's as raster-list
r.list <- list()
for(i in 1:length(DHM_required$DHM_URL)){  
  r.list[[i]] <- raster(DHM_required$DHM_URL[i])  
} 
```

As shown in figure x below, only the DEM-rasters of the runned area have been imported automatically. The points additionally show all the x- and y- coordinates that have been tracked by the athlete among all activities. They are all covered by the imported DEM.

```{r echo=FALSE, fig.cap="Figure x: Digital elevation model (DEM) of the ovservation area with x and y coordinates of all analysed tracks", message=FALSE, warning=FALSE}
# Create a Rasterlayer from raster-list
m <- do.call(merge, r.list)

# Visialize imported data
plot(m)  # This should exactly cover the tracks used
points(full_sf$x, full_sf$y)   # Add all points to the DEM-Graphic
```

Finally, the height information from the DEM could be extracted for every single x- and y-coordinate of the tracks to be analyzed. By joining the height information from the DEM with the coordinates of the tracks the investigation about accuracy of the elevation measurement by the pressure became possible. In the results, the absolute heights of both, the DEM and the pressure measurement are shown, as well as the height difference between the two data sources. Since the pressure measurement leads to constant fluctuations in the altitude information (see chapter [3.1.1]{.ul}), the altitude from the DEM is considered more reliable and is used for subsequent evaluations.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Reading of other tracks all over Switzerland will result in the import of the required rasters above!! :)

# Extract elevation information from rasterlayer
data.matrix <- as.data.frame(rasterToPoints(m))

# Create identical names for join in DEM source:
names(data.matrix)[names(data.matrix) == "x"] <- "x_round"
names(data.matrix)[names(data.matrix) == "y"] <- "y_round"

# create rounded x and y as join-key => every two meters, always odd values!
full_sf$x_round <- as.numeric(2 * round(full_sf$x/2) + 1)  
full_sf$y_round <- as.numeric(2 * round(full_sf$y/2) + 1)


# Join elevation 
full_sf <- left_join(full_sf, data.matrix, by = c("x_round", "y_round"))

#####  dplyr::rename  => change layer to altitude_DEM (andd)

# Calculate altitude difference between pressure measurement and DHM elevation (layer):
full_sf$altitude_diff <- full_sf$layer - full_sf$enhanced_altitude
```

#### 2.2.2 Compare recorded distance with Euclidean Steplength

Not only the altitude but also the distance recorded by the watch was compared with own calculations. Therefore, the Euclidean step length was calculated with data from the prepared data.frame "full_sf" where the GPS tracks were converted into the Swiss national coordinate system *CH1903+ LV95* (See chapter 2.2.1). 
As a preparation, for each runned track the whole distance (Track_Distance), Euclidean Steplength (steplength), the whole runned distance by a given timepoint (steplength_sum), highdifference calculated with data from DEM (highdiff) and highdifference up, if highdiff >0 and down if highdiff <0 was calculated. 

The distance between the distance derived by the sportwatch and distance calculation by Euclidean distance was compared and presented later in the results.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# calculate Euclidean step length and speed
#steplength_sum is distance runned at specific timepoint for each track 
full_sf <- full_sf %>%
  group_by(activity_ID) %>%
  mutate(
    Track_Distance = max(distance),
    steplength = sqrt((x - lead(x))^2 + (y - lead(y))^2),
    steplength_sum = lag(cumsum(replace_na(steplength,0))),
    hightdiff = lead(layer) - layer, # laver is the elevation from the DEM!
    hightdiff_up = ifelse(hightdiff > 0, hightdiff, 0),
    hightdiff_down = ifelse(hightdiff < 0, hightdiff, 0)
  )

#compare distance and steplength_sum

#generate random tracks to check
random_tracks <-sample(full_sf$activity_ID, 6)

#### Distance (absolute or in percent) instead of comparison

full_sf$distance_diff <- (full_sf$steplength_sum - full_sf$distance) / full_sf$Track_Distance
```

#### 2.2.3 Rolling windows

In this chapter a rolling window with different k-values was generated in order to smooth the time step-values. In figure x violin plots from all tracks from athlete 1 are shown. Further in figure x all smoothed speed curves are collected and compared to each other. Enhanced_speed_00 is the curve without rolling window (enhanced_speed multiplied by 3.6 to get the unit km/h). Since low values (around 0 km/h) are important to pick out the breaks in further steps, these values must not be lost. For this reason k=3 was defined and used for further calculations.  

```{r message=TRUE, warning=FALSE, include=FALSE, paged.print=TRUE}

# speed from tracker in km/h
full_sf$enhanced_speed_kmh <- full$enhanced_speed *3.6

# smooth speed and heart rate by rollmenans

full_sf <- full_sf %>%
  group_by(activity_ID, athlete) %>%
  mutate(
    enhanced_speed_00 = enhanced_speed_kmh,
    enhanced_speed_02 = rollmean(enhanced_speed_kmh, k = 2, fill = NA, allign = "left"),
    enhanced_speed_03 = rollmean(enhanced_speed_kmh, k = 3, fill = NA, allign = "left"),
    enhanced_speed_05 = rollmean(enhanced_speed_kmh, k = 5, fill = NA, allign = "left"),
    enhanced_speed_10 = rollmean(enhanced_speed_kmh, k = 10, fill = NA, allign = "left"),
    enhanced_speed_50 = rollmean(enhanced_speed_kmh, k = 50, fill = NA, allign = "left")
  )

full_sf <- full_sf %>%
  group_by(activity_ID) %>%
  mutate(Track_Distance = max(distance)) %>%
  ungroup()

#check moving windows for track distances > 10km
full_sf %>%
  filter(Track_Distance > 10000) %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

#check moving windows for track distances < 10km
full_sf %>%
  filter(Track_Distance < 10000) %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

#generate one random track in order to check (for athlete 1)
random_track <-sample(full$activity_ID, 1)
```

```{r echo=FALSE, message=TRUE, warning=FALSE, paged.print=TRUE, fig.cap="Figure x: Frequency of speeds in km/h depending on rolling window size"}
######## Use this graph for report
full_sf %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

```

```{r echo=FALSE, message=TRUE, warning=FALSE, paged.print=TRUE,fig.cap="Figure x: Speed in km/h of a random track depending on rolling window size"}

speeds_rW <- full_sf %>%
  filter(activity_ID == random_track) %>%
  gather(smoother, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(timestamp, speed, col = smoother)) +
  geom_line() +
  theme_bw()
ggplotly(speeds_rW)
#### USe k = 3 for further analysis
```

#### 2.2.4 Define static and moving points

This chapter is about distinguishing the pauses from the run parts.  Since it is also possible to walk around during a pause, a threshold in speed has to be defined. Therefore a histogram with all speed values from Athlete 1 was generated. From visual inspection and together with the data from the violin plots from chapter x, the threshold was defined at <= 3.25 km/h. Therefore all speed values <=3.25km/h were labeld as static = TRUE.   

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}


#Remove “static points”

#Define threshold 

#plot trajectories with behavior pattern (static/moving) for Athlete 01 
#a) check data: histogram for speed for different tracks 


speed_mean <- mean(full_sf$enhanced_speed_03, na.rm=TRUE)

full_sf %>% 
  ggplot()+
  geom_histogram(aes(enhanced_speed_03), binwidth = 0.5)+
  geom_vline(aes(xintercept=speed_mean),linetype="dashed", color="blue")+
  geom_vline(aes(xintercept=3.25),linetype="dashed", color="red")+
  geom_text(x=3.25, y=-30, label="threshold", color="red")+#set threshold = 3.25 from visual context
  geom_text(x=speed_mean, y=-30, label="mean", color="blue")+
  theme_bw() +
  labs(x = "\nspeed in km/h (smoothed by rolling window k = 3)", y = "frequency\n")

### Define as static when speed < 3.25 km/h


##all points which are < than the threshold value of 3m/s of enhanced_speed_kmh are defined as static (moving)
#### Save in full_sf instead of tracks
full_sf <- full_sf %>% 
  ungroup() %>%
  mutate(static = enhanced_speed_03 <= 3.25)
```

#### 2.2.5 Define outliers

Since the goal for Research Question 3 is a generation of a time prediction model, the time system boundaries must be defined. Therefore, the time prediction must be in the range of the existing data. For this, the real running time was calculated first (from static = False) and the maximum race time was read out. For the model 10% to the maximum value from the existing is allowed. In the case of athlete 1 this means that the time prediction must not be longer than 3h. 

To further detect outliers of the existing data / tracks an is_outlier function was created that will return a boolean TRUE/FALSE. Therefore, an often used rule was applied, which says that a value is an outlier if a value is more than 1.5 x IQR (interquartile range)above the upper quartile (Q3) or below the lower quartile (Q1). In other words, lower outliers are below Q1-1.5 x IQR and upper outliers are above Q3+1.5 x IQR. 


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#calculate duration of each track and
#calculate duration of each track without breaks and
#calculate duration of breaks per track

full_sf_filtered <- full_sf %>% 
  group_by(activity_ID) %>% 
  mutate(duration_whole_track = as.numeric(difftime(max(timestamp), min(timestamp), units = "hours"))) %>% #in hours
  ungroup() %>% 
  group_by(activity_ID, static) %>% 
  filter(static == FALSE) %>% 
    mutate(duration_moving = sum(timelag, na.rm = TRUE) / 3600) %>% 
  ungroup() %>% 
  mutate(duration_breaks = (duration_whole_track - duration_moving), 
         duration_breaks_min = (duration_breaks*60))


full_short <- full_sf_filtered %>% 
  group_by(activity_ID) %>% 
  summarize(duration_track = mean(duration_whole_track), 
            duration_moving = mean(duration_moving), 
            duration_breaks = mean(duration_breaks), 
            duration_breaks_min= mean(duration_breaks_min)) %>%
  ungroup() 

  ggplot(full_short)+
    geom_col(aes(x=(reorder(activity_ID,-duration_moving)),y=duration_moving))+
    coord_flip()+
    theme_bw()+
    ylim(0,3)+
    labs(title="moving duration of the tracks",
        x ="activity ID", y = "time in hours")
  
   
min(full_short$duration_moving)
max(full_short$duration_moving)

max(full_short$duration_moving)*1.1 #consider 10% deviation --> 3.01h 

  
#tracks duration beween 0.235 h and 2.77h for athlete 1  --> model prediction should not consider routes >3h

#investigation for outliers

#function for outlier hat will return a boolean TRUE/FALSE if the value passed to it is an outlier: 
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}
```

### 2.3 Model individual time requirement <a name="subparagraph4"></a>

To model the time requirement of an activity, distance and altitude differences (upwards and downwards) were used as predictors in a first step. The distance derived by the sportwatch was used as a comparison with the euclidian distance did not show any relevant differences. In contrast the altitude differences were calculated based on the DEM as these method seems to produce more accurate results. Subsequently, the model was refined by including fatigue (based on the time estimation of the first model). Fatigue was included because the first model led to a systematic overestimation of the time needed for short activities, whereas the time needed for longer activities was typically underestimated.

1.  Simple Model: Before starting with a multimodel inference (multiple regession), the activities were segmented in increments of one minute. Using multimodel inference, all predictors were checked for their importance. Subsequently, the model with the highest AICc value (lowest delta AICc respectively) was selected to model the individual time requirement of athletes.


2.  Exhaustion Model: Due to exhaustion, the simple model leads to an overestimation of the time requirement for short activities and tends to underestimate it for longer activities. For this reason, the fatigue component is added to the model. For this purpose, the time difference between prediction and measurement of each track is used and calculated using a linear model, based on the duration of the prediction. This results in a lower time requirement for short activities and a higher time requirement for longer activities. The slope of the linear model increases due to this correction, but by reducing the intercept, the time requirement decreases for short activities.

#3. Results

## 3.1. Compare data from sportwatch and calulations

### 3.1.1 Altitude by Pressure vs. Altitude from DEM <a name="subparagraph1"></a>

As described in the methods the accuracy of the pressure measurement by sportwatches had to be evaluated. Therefor the pressure derived altitude from the watch was compared to a high resolution digital elevation model (DEM). The analysis includes a comparison of the absolute altitude measurement in metres above sea level and the altitude difference between the two data sources.

In the case of absolute altitude, there are only a few major differences between the two data sources. For example, activity 9 shows a comparatively large deviation at the beginning, see Figure x.

```{r echo=FALSE, fig.cap="Figure x: Comparison of elevation calculation based on DEM and measurement based on air pressure of the sportwatch", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
# Test accuracy of elevation measurement
full_sf %>%
  st_drop_geometry() %>%
  dplyr::select(activity_ID, sportwatch_pressure = enhanced_altitude,
         DEM = layer, timestamp) %>%
  gather(source, value, sportwatch_pressure, DEM) %>%
  ggplot(aes(timestamp, value, col = source)) +
  geom_line() +
  facet_wrap(~activity_ID, scales = "free", ncol = 3) +
  theme_bw()+
  labs(x = "\ntimestamp hh:mm", y = "altitude measurement m a.s.l.\n")+
  theme(legend.position = "bottom")

```

The height difference of both data sources shows more or less permanent fluctuations. It should be noted that a constant variation of the altitude in total leads to a greater altitude difference than realistically completed. These fluctuations are caused on the one hand by the inaccuracy of the pressure measurement of the clock and on the other hand by the inaccuracy of the GPS fixes, which in turn influence the altitude based on the DEM. Based on this data, it is not possible to assess with certainty which data source is the more reliable. However, it is assumed that the GPS fix of the watch is more accurate than the pressure measurement and that the data from the DEM are therefore more reliable. The deviations between the two data sources are shown in figure x.

```{r echo=FALSE, fig.cap="Figure x: Difference between of elevation calculation based on DEM and measurement based on air pressure of the sportwatch", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
full_sf %>%
  ggplot(aes(timestamp, altitude_diff, col = altitude_diff)) +
  geom_point() +
  facet_wrap(~activity_ID, scales = "free", ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(x = "\ntimestamp hh:mm", y = "altitude difference (DEM - pressure measurement) in m\n")
```

###3.1.2 Tracked distance vs. calculated distance <a name="subparagraph1"></a>

The comparison between the recorded distances and those from Eucliedean Distance shows that..


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(full_sf, aes(distance_diff))+
  geom_histogram(binwidth = 0.001)+
  labs(x= "\ndifference between data from sportwatch and euclidian distance\ncalculation of every timestep among all tracks", y = "frequency\n") +
  theme_bw()+
  scale_x_continuous(labels = scales::percent)
```

### 3.2 Research Question 1 <a name="subparagraph1"></a>

#### 3.2.1 Visualization of the tracks on map

There are various packages in R that can be used to visualize location data. In the following, some selected tracks are shown by means of such visualizations to provide an overview about the analyzed raw data . We have restricted ourselves to common packages such as leaflet() and ggmaps().

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE, fig.cap="Figure x:"}
# ggplot without map => not included in report
library(tidyverse)
ggplot(full_sf, aes (x, y, col = enhanced_speed_kmh)) +
  geom_point() +
  facet_wrap(~activity_ID, scales = "free") +
  theme_bw()
```

The Package leaflet offers an interactive option for visualising activities on a map, which is particularly impressive visually. In the following figures x and x, two selected activities are visualised using leaflet. By zooming in, the accuracy of the GPS fixes can be analysed visually.

For example, if the user zooms into a forested region (between the locations of Krummenau and Ebnat-Kappel) in figure x, it shows that there is sometimes a large deviation between the GPS fixes and the paths.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Visualization of a track on a map using package leaflet"}

library(leaflet)

# Example track 1
m1 <- full %>%
  ungroup() %>%
  filter(activity_ID ==1) %>%
  dplyr::select(position_long, position_lat) %>% 
  as.matrix() %>%
  leaflet(  ) %>%
  addTiles() %>%
  addPolylines( )
m1
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Visualization of a track on a map using package leaflet"}

# Example track 6
m2 <- full %>%
  ungroup() %>%
  filter(activity_ID ==3) %>%
  dplyr::select(position_long, position_lat) %>% 
  as.matrix() %>%
  leaflet(  ) %>%
  addTiles() %>%
  addPolylines( )
m2
```

Another possibility to visualize the activities on a map is offered by the package ggmap. It is based on the ggplot package, but allows the inclusion of map material. Due to the functionality of ggplot, speeds, for example, can be included in the visualization relatively easily by choosing color scales. The following figures x and y show an activity and the speed of the athlete as well as the altitude. When comparing the two graphs, it can be seen that the speed in areas with increasing altitude is lower than in the remaining areas. This shows the relevance of the altitude difference for the development of a model to predict the time required for activities.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Visualization of a track on a map using package ggmap. The color scale represents the speed of the athlete."}
# Use ggmap for visualization
track1_3 <- full %>%
  filter(activity_ID == 1) %>%
  mutate(speed = enhanced_speed *3.6)

library(ggmap)

myLocation <- c(min(track1_3$position_long), min(track1_3$position_lat), 
                max(track1_3$position_long), max(track1_3$position_lat))  # Position definieren für Karten

myMap <- get_stamenmap(bbox=myLocation, maptype="terrain", crop=TRUE, zoom = 13)  # Kartenimport

ggmap(myMap) +
  geom_point(data = filter(track1_3, speed > 3.25), 
             aes(position_long,position_lat, col = speed))
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Visualization of a track on a map using package ggmap. The color scale represents the elevation at the coordinate."}

track1_3 <- full %>%
  filter(activity_ID == 1) %>%
  mutate(speed = enhanced_speed *3.6)

library(ggmap)

myLocation <- c(min(track1_3$position_long), min(track1_3$position_lat), 
                max(track1_3$position_long), max(track1_3$position_lat))  # Position definieren für Karten

myMap <- get_stamenmap(bbox=myLocation, maptype="terrain", crop=TRUE, zoom = 13)  # Kartenimport

ggmap(myMap) +
  geom_point(data = filter(track1_3, speed > 3.25), 
             aes(position_long,position_lat, col = enhanced_altitude))
```

#### 3.2.2 Variables affecting the time requirement (shiny)

tophal

To answer research question 1, an exploratory data analysis (EDA) of the collected tracks was first conducted.

The Shiny app was programmed to browse through the different tracks of an athlete. Via dropdown menu it is possible to choose between the different tracks:

```{r echo=FALSE, fig.cap="Figure x:", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
full_long <- gather(full_sf, type, value, layer, heart_rate, enhanced_speed_03)

>>>>>>> a52b6104feca5832167ea8d4a43b57449d467bce
library(shiny)
library(plotly)

#select plots which show graphically how the variables speed, altitude difference and heart rate behave.

#create function for generation of plots

plot_speed_altitude_heartrate_function <- function(track_ID){
  
  full_long %>%
  filter(activity_ID == track_ID) %>%
  ggplot(aes(timestamp, value, col = type)) +
  geom_line()+
  geom_point() +
  facet_wrap(~type, scales = "free",ncol = 1) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "\ntimestamp in hh:mm", y = "Value\n")
  
}

#count number of tracks (unique activity ID)
number_of_tracks <- length(unique(full_sf$activity_ID))

all_plots_speed_altitude_heartrate <- list() #create empty array

#save plots in array "all_plots_speed_altitude_heartrate"
for (track_ID in 1:number_of_tracks) {
    all_plots_speed_altitude_heartrate[[track_ID]]<-plot_speed_altitude_heartrate_function(track_ID)
  }
#create userinterface ui with shiny
ui <-shinyUI(fluidPage(selectInput("selectPlot", "Choose desired track", choices=1:number_of_tracks), plotlyOutput("plot")))

server <- shinyServer(function(input,output){      
  output$plot <- renderPlotly({
    all_plots_speed_altitude_heartrate[[strtoi(input$selectPlot)]]
  })
})

#get userinterface 
shinyApp(ui,server)
```

#### 3.2.3 Summary of tracks

A summary of all analyzed tracks is shown in table x. This shows that activities in a distance range from x to y kilometers were used. The positive altitude difference per track were between x and y m, the negative altitude difference was measured between x and y m. It also shows the average pace (time required per kilometer of distance). The pace seems to be dependent on the distance and the altitude differences in the sense of an explorative data analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#### Use values from full_short! (andd)
library(knitr)
zf <- full_sf_filtered %>% #zf because of "Zusammenfassung" in german
  st_drop_geometry() %>%
  group_by(activity_ID) %>%
  summarise(
    date = date(timestamp[1]),
    distance = max(distance, na.rm = TRUE),
    time = sum(timelag, na.rm = TRUE)/60,
    hightdiff_up = sum(hightdiff_up, na.rm = TRUE),
    hightdiff_down = sum(hightdiff_down, na.rm = TRUE),
    mean_pace = time / distance * 1000
    )
kable(zf[order(zf$mean_pace, decreasing = F),], caption = "Table x: Summary of all tracks, ordered by descending mean pace")
```

### 3.3 Research Question 2 <a name="subparagraph1"></a>

In this chapter the goal was to indicate specific behavioral patterns (e.g. speed, breaks, or similar) in the sports data and when they occur.

#### 3.3.1 Moving/breaking <a name="subparagraph1"></a>

tophal

In order to produce a model in chapter 3.5, some framework conditions must first be established. For this purpose, the length of the tracks, the actual running time and the breaks per track were identified.

To identify possible outliers,

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Identification of outliers"}
full_short %>%
  mutate(outlier = ifelse(is_outlier(duration_moving), activity_ID, as.numeric(NA))) %>%
  ggplot(aes(x = activity_ID, y = duration_moving)) +
    geom_boxplot() +
    geom_text(aes(x = 10.25, label = outlier), na.rm = TRUE, hjust = -0.2)+
  theme_bw()+
    labs(title="moving duration of the tracks",
        x ="", y = "time in hours\n") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())
#track 1 has to be controlled for possible outliers
```

The statistical analysis showed that track 1, 9, 14 should be removed and that the running time for the prediction model should not be longer than 3h (system boundary).

#### 3.3.2 Heart rate patterns<a name="subparagraph1"></a>

tophal

According to Such and Meyer (2010) the maximum heart rate (HRmax) is calculated as 220-age for running and 200-age for biking (rule of thumb). Also, women-specific calculations such as HFmax = 206 - (age x 0.88) (also rule of thumb) exist (Gulati et al). Since only data from male athletes exist in this work, it will not be used further. Therefore, it is important that the data used is up to date or at leas approximately the same as the current age.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Identification of running sequences with heart rates higher then recommended"}
#max heart rate --literature athlete 1 
#date of athlete 1
birth_athelete1 <- as.Date('1991-11-16')
now <- Sys.time()

age_a1 <- time_length(difftime(now, birth_athelete1), "years")

max_heart_rate_literature <- 220-age_a1
high_rate <- (max_heart_rate_literature*0.86)
paste("The indivudial is ", round(age_a1,0), " years old", " which leads to a maximum heart rate of ", round(max_heart_rate_literature,0), " BPM and a good training value for this indivudual would be around ", round(high_rate,0)," BPM", sep = "")

tracks_heart <- full_sf %>% 
  group_by(activity_ID) %>% 
    mutate(high = heart_rate > (high_rate), na.rm = TRUE)


#make plot with heart_rate normal (training) and high rate
ggplotly (tracks_heart%>%
  ggplot(aes(x, y, col=(high)))  +
  geom_path() +
  geom_point() +
  coord_fixed() +
  theme_bw()+
  theme(legend.position = "right")+
  facet_wrap(.~activity_ID)+
  scale_color_manual(values=c("#E69F00", "red")))
```

According to the American Heart Association, the following target heart rate are recommended:

-   Moderate exercise intensity: 50% to about 70% of your maximum heart rate.

-   Vigorous exercise intensity: 70% to about 85% of your maximum heart rate.

For beginners the target heart rate will even be smaller. It is the athlete's choice which training level to select. However, it should not be higher than 85%.

Therefore

### 3.5 Research Question 3 <a name="subparagraph1"></a>

andd =\> Done

In this chapter the results about the research question 3 are presented. First, a simple model for time estimation was build and the accuracy of the estimations was calculated. Then, exhaustion was taken into account by a model extension. Additionally, a new jogging route (not covered in the analyzed tracks) was chosen. Before absolving this route, the time requirement was estimated using the exhaustion model. Finally, the time estimation of this new route was compared to the measurement.

#### 3.5.1 Simple model

As described in the methods all activities were segmented in increments of one minute. For these increments the distance and the altitude differences (upwards and downwards) were calculated. These increments were then used to model the time requirement based on the distance and the height differences.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
full_sf_min <- full_sf_filtered %>%
  st_drop_geometry() %>%
  group_by(activity_ID) %>%
  mutate(Track_Distance = max(distance)) %>%
  group_by("time" = cut(timestamp, "1 min")) %>%
  summarise(
    distance = sum(steplength, na.rm = TRUE),
    Track_Distance = Track_Distance[1],
    hightdiff_up = sum(hightdiff_up, na.rm = TRUE),
    hightdiff_down = sum(-hightdiff_down, na.rm = TRUE),
    hightdiff_balance = hightdiff_up + hightdiff_down,
    time_run = sum(timelag, na.rm = TRUE),
    mean_pace = time_run / distance * 1000 / 60
  ) %>%
  mutate(vertical_direction = ifelse(hightdiff_balance > 2, "up",
                                      ifelse(hightdiff_balance < -2, "down",
                                             "horizontal")))

# Create a linear Model to calculate time by dist, hightdiff:
min.model <- lm(time_run/60 ~ 0 + distance + hightdiff_up + hightdiff_down, data=full_sf_min)
options(na.action="na.fail")
allmodels.min <- dredge(min.model)   # Create all possible models

avgmodel.min<-model.avg(get.models(dredge(min.model,rank="AICc"),subset=TRUE))
summary(avgmodel.min) # All predictors are highly significant!

avgmodel.min$coefficients

# Extract coefficients from the best Model [1]
distance_param_min <- allmodels.min$distance[1]
up_param_min <- allmodels.min$hightdiff_up[1]
down_param_min <- allmodels.min$hightdiff_down[1]
options(digits = 4)
```

All models of the multimodel inference are shown in table x. The model on the top uses all degrees of freedom (4) which means that the distance as well as the heigthdifferences are used to model the time requirement. As the model on the top shows the lowest AICc value, this model is further used. Model parameters are as follows:

-   Distance affects the time estimation by `r distance_param_min` minutes per meter of distance which corresponts to `r distance_param_min * 1000` minutes per kilometer.
-   Positive height difference affects the time estimation by `r up_param_min` minutes per meter of height difference.
-   Negative height difference affect the time estimation by `r down_param_min` minutes per meter of height difference.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

kable(allmodels.min[,1:7], digits = c(4,4,4,0,0,0,0),caption = "Table x: Models derived by multimodel inference. The model on the top uses all degrees of freedom (df) and shows the lowest AICc value corresponding to a delta AICc of 0 whereas the other models are less relevant (showing higher delta AICc values).")
```

The above described simple model without exhaustion predicts the time requirement for all activities with some deviation to the measured time requirement. As figure x shows, the time estimation meets the measured time requirement for some activities pretty well, whereas the time requirement especially for long activities (measured time requirement) are mostly underestimated. This shows the effect of the exhaustion.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Time estimation of the simple model without exhaustion in comparison with measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement."}
 
# Estimate time requirement based on above created model parameters
zf$time_estimated_min <- zf$distance *distance_param_min + zf$hightdiff_up * up_param_min + zf$hightdiff_down * down_param_min

# Plot measured and estimated time: Model underestimates time requirement for long activities,
#  and overestimates for short activities (see following plots)
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_min, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, estimated) %>%
  ggplot(aes(reorder(activity_ID, -distance), value, col = type)) +
  geom_point() +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "time requirement in minutes\n")

```

In addition to the absolute time requirement in minutes, the relative difference between the simple model and the measurement is shown in figure x on a relative scale (deviation between model and measurement in percent). The upper part of the graph shows the measured distance per activity, the bottom shows the deviation of the time in percent. It is clearly demonstrated, that the simple model overestimates the time requirement for short activities (up to around 5 kilometers) and generally underestimates the time requirement for longer activities. As this graph does not take height differences into account, some of long-distance tracks are predicted pretty well. Still, there seems to be a systematical error in the model.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Relative deviation between time estimation of the simple model without exhaustion and the measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement. The upper part shows the distance per track in meters, the bottom part shows the deviation."}

# Exhaustion can be seen here
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_min, distance, hightdiff_up, hightdiff_down) %>%
  mutate(deviation = estimated / measured -1) %>%
  ggplot(aes(reorder(activity_ID, -distance), deviation, fill = deviation)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_label(aes(label = paste(round(deviation,2)*100, "%", sep = "")), fill = "white")+
  geom_xsidepoint(aes(y = distance)) +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "relative under- and overestimation of time requirement // distance")
```

#### 3.5.2 Exhaustion model

To correct the systematical error of the above presented model, the effect of exhaustion was calculated using a linear model. First, the absolute time difference between estimation and measurement was calculated. Subsequently, this difference was modeled by a linear model using the estimated time as predictor of the exhaustion. This leads to the following results:

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Adjust Model from above with exhaustion: Calculate the time difference between measurement and base
#  model by the estimated time of the model
zf$time_diff <- - zf$time_estimated_min + zf$time   # Time estimated - time measured
adjust.model <- lm(time_diff ~ time_estimated_min, data = zf)
summary(adjust.model)  # Intercept is significant, time estimation too. R2 = 0.61.
adjust.model$coefficients

# Create exhaustion coefficient and intercept
exhausing_coeff <- as.numeric(adjust.model$coefficients[2])
exhausting_interc <- as.numeric(adjust.model$coefficients[1])

sum.ex <- summary(adjust.model)
options(digits = 2)
```

-   The intercept of the simple model is to be corrected by `r as.numeric(adjust.model$coefficients[1])` minutes. This will lead to a lower time estimation for tracks with a low estimation in time requirement

-   The slope of the simple model will be adjusted by an addition of `r as.numeric(adjust.model$coefficients[2])` minutes per minute of estimated time requirement.

The summary of this linear exhaustion model is presented in table x. The r squared of the model is `r sum.ex$r.squared` which means that there is a strong relationship between the time difference and the estimated time of the activity. Still, this adjustment with exhaustion is a linear model with a limited scope of validity. Nevertheless, the exhaustion model seems to predict the time requirement for activities more accurate among most of the tracks. While deviations of the simple model were between -21 and +12%, these deviations (figure x) were reduced to a maximum of -11 % and + 13%. Furthermore the differences no more seem to be systematical.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Relative deviation between time estimation of the exhaustion model and the measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement. The upper part shows the distance per track in meters, the bottom part shows the deviation."}
# Include Exhaustion into model:
zf$time_estimated_full <- zf$distance *distance_param_min + zf$hightdiff_up * up_param_min + zf$hightdiff_down * down_param_min + zf$time_estimated_min * exhausing_coeff + exhausting_interc

zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  mutate(deviation = estimated / measured -1) %>%
  ggplot(aes(reorder(activity_ID, -distance), deviation, fill = deviation)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_label(aes(label = paste(round(deviation,2)*100, "%", sep = "")), fill = "white")+
  geom_xsidepoint(aes(y = distance)) +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "relative under- and overestimation of time requirement // distance")

```

#### 3.5.3 Comparison of the Models and the Measurement

Figure x additionally shows the measured time requirement per track in descending order as well as the prediction of both models. The exhaustion model seems to predict the time requirement more precise for the most of the activities.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: Measured time requirement and both model estimations per track, ordered by descending distances."}
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated_simple = time_estimated_min,  estimated_exhaustion= time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, estimated_exhaustion, estimated_simple) %>%
  ggplot(aes(as.factor(reorder(activity_ID, -distance)), value, fill = type)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7)+
  theme_bw() +
  labs(x = "\nactivity-ID", y = "time requirement in minutes\n")
```

```{r include=FALSE}
# These were just further analyses that werent included in the report...
zf %>%
  dplyr::select(time, simple = time_estimated_min, exhaustion = time_estimated_full) %>%
  gather(model, value, simple, exhaustion) %>%
  ggplot(aes(time, value, col = model)) +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  xlim(0,175) +
  ylim(0,175) +
  geom_point()+
  theme_bw() +
  labs(x = "time measured", y = "time estimated")

# There ist still deviation but less systematical!
zf %>%
  dplyr::select(time, simple = time_estimated_min, exhaustion = time_estimated_full) %>%
  gather(model, value, simple, exhaustion) %>%
  ggplot(aes(time, value, col = model)) +
  geom_abline(intercept = 0, slope = 1, col = "black") +
  xlim(0,75) +
  ylim(0,75) +
  geom_point()+
  theme_bw() +
  labs(x = "time measured", y = "time estimated")
  

# Exhaustion Model represents measurement better than simple model without exhaustion.
zf %>% 
  dplyr::select(activity_ID, measured = time, simple = time_estimated_min, exhaustion = time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, simple, exhaustion) %>%
  ggplot(aes(distance, value, col = type)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

#### 3.5.4 Validation of the Exhaustion Model

andd =\> done!

```{r include = FALSE}
Running_Dist <- 13540     # Distance of a route
Running_up <- 712         # Hightdifference up
Running_down <- 488       # Hightdifference down

valid_time_simple <- Running_Dist * distance_param_min + 
  Running_up * up_param_min + 
  Running_down * down_param_min 
valid_time_simple         # Gives time estimate with simple model

valid_time_exhaustion <- valid_time_simple + exhausting_interc + valid_time_simple * exhausing_coeff
valid_time_exhaustion     # Gives time estimate with exhaustion model

effective_time <- 93.2
options(digits = 0)
```

In order to validate the exhaustion model, athlete 1 defined a route which was not covered in the observation area of the analyzed tracks. This route can be specified as follows (according to *SchweizMobil*):

-   Distance of `r as.integer(Running_Dist)` meters in total

-   A total of `r Running_up` meters of positive height difference

-   A total of `r Running_down` meters of negative height difference

Based on these model inputs, the simple model calculates a time requirement of `r valid_time_simple` minutes, whereas the exhaustion model leads to a time estimation of `r valid_time_exhaustion` minutes. Athlete 1 took `r effective_time` minutes to complete the course in the test. Thus the prediction of the exhaustion model is much closer to the measurement than that of the simple model. Nevertheless, it must be noted, that this test with a sample size of only one track is definitely to little to to enable statements about the accuracy of the model.

###  3.5.5 Visualization of Exhaustion Model

Assuming a range of validity of the model with distances between 3 and 32 kilometers and positive altitude differences between 0 and 1000 meters, the time required for activities can be classified on the basis of the model and displayed on a graph. Figure x shows such a graph, where the predicted time demand can be read out on the vertical z-axis under different conditions (distances and altitude differences). However, the extremely broad scope of this observation means that the time prediction can only serve as a rough basis for planning. Effective times will deviate, among other things, due to the non-linearity of fatigue and environmental parameters. These aspects will be examined in more detail in the discussion.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure x: 3D-Model of the time estimation by the exhaustion model depending of various distances and hight differences. Hight differences are always calculated for both, upwards and downwards."}
# Show the time estimation of different distances and hightdifferences as a 3D plot
timetable <- data.frame(
  Distance_m = rep(seq(3000 , 32000,  500), each = 11),
  hight_difference_m = rep(seq(0,1000, 100), 59)
)

timetable$simple_model_min <-timetable$Distance_m *distance_param_min + 
  timetable$hight_difference_m * up_param_min + timetable$hight_difference_m * down_param_min

timetable$exhaustion_model_min <- round(timetable$simple_model + exhausting_interc + exhausing_coeff * timetable$simple_model,0)

timeplot <- plot_ly(timetable, x = ~Distance_m, y = ~hight_difference_m, z = ~exhaustion_model_min,
                    color = ~exhaustion_model_min)
timeplot
```

## 4 Discussion <a name="discussion"></a> {#discussion}

In the following subchapters, the accuracy of the data and the model are discussed. Additionally, the limits of the model to predict the individual time requirement for an athlete at a given route are commented. This leads to the final conclusion about existing problems in the prediction of the time requirement for sporting activities as well as topics to be studied further.

### 4.1 Accuracy of the Data a name="subparagraph2"></a>

The analyses showed a relatively high accuracy of the data in many areas. For example, the distance calculated directly by the sports watch as well as the speed agree very precisely with the calculation via the euclidean distance, with a maximum deviation of 1%.

In the area of altitude differences, there were larger differences between the measurement from the watch, which is based on air pressure, and the DEM. For this reason, the height difference based on the GPS-fixes and a DEM with high resolution was used for the modeling of the time demand. This approach could lead to problems in a larger study area, as the amount of data to be imported increases proportionally with the extension of the study area. Furthermore, the method developed here for importing the DEM is limited to Switzerland, as the underlying elevation model is only available for Switzerland from the source used. Alternative elevation models are available, but would require some additional programming effort.

Finally, however, all relevant parameters depend on the accuracy of the GPS fixes. Figure x shows that the deviations between the GPS fixes and the paths can sometimes be large. A visual analysis of tracks on the map revealed large differences, especially in areas with forest cover. These differences influence all predictors of the model (distance directly and altitude differences indirectly via the DEM).

### 4.2 Limits and Accuracy of the Model a name="subparagraph2"></a>

andd

In addition to the imprecision of the data, the accuracy of the model and its scope must be questioned. After creating a simple model without fatigue, an extended model was created that can at least partially represent athlete fatigue. This improved the predictions, especially for very short and very long activities. Nevertheless, there are still fluctuations in a range of maximum 15% compared to the effective measured values.

The fatigue model is also a linear model that can only reflect the athlete's situation within a certain range of validity. A validation of the fatigue model with an additional activity showed a high accuracy of the prediction, but does not allow a conclusive statement about the quality of the model. The authors assume that the model can predict the time required for activities in a distance range of 5 to 20 km and an altitude difference of 200 to 800 m relatively accurately.

Finally, it should be mentioned that the results described here refer to the evaluation of a single athlete. Data from a second person are available and were examined in a separate run. This separate run showed that the method can also be applied to other persons. This means that other model parameters are generated and therefore the time prediction also changes. The goal of a time prediction based on the individual fitness level could thus be fulfilled. The methodology used here can be transferred to other athletes as desired, although the .fit files used must be exchanged.

### 4.3 Problems / Further studies<a name="subparagraph2"></a>

tophal

Factors which influence time requirement (and heart rate), include not only the route and its path circumstances, but also

-   Air-temperature / weather conditions

-   Age

-   Having disease such as cardiovascular disease, high cholesterol, diabetes etc.

-   Medications

-   Smoker/non-smoker

-   Emotions

## x References
