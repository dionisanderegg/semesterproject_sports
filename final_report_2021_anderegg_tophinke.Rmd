---
title: "Modelling the individual time requirement for sport activities using previous GPS-Tracks"
author: "Anderegg Dionis, Tophinke Alissa"
date: " 04/07/2021"
output:
  html_document:
    toc: true
    theme: united
    toc_depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
runtime: shiny
bibliography: references.bib
fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, include=FALSE}
library(rgdal) 
library(sf)
library(tidyverse)
library(lubridate)
library(zoo)
library(purrr)
library(base)
library(ggpubr)

if(!requireNamespace("remotes")) {
    install.packages("remotes")
}
remotes::install_github("grimbough/FITfileR")

library(FITfileR)
library(gdata)
library(plotly)
library(ggside)

#Packages for DEM import and handling
library(raster)
library(tiff)

library(MuMIn)

```

## Abstract

In this semester project, sports tracks are analysed using data from "Garmin" devices to identify specific behavior patterns in jogging activities. Exploratory data analysis was used to examine the available data and identify the most important parameters, which were then presented in descriptive plots. Therefore, the time required for jogging activities is modeled based on the individual fitness level, so that a user-specific forecast of the time required for activities is possible. Two different multiple regession models (Simple Model/ Exhausting Model) were set up, with the latter yielding better forcasts by taking fatigue into account. Model inputs are distance and altitude differences (upwards and downwards).

# Introduction

Sportwatches are a popular tool to monitor and regulate training activities among both professional and amateur athletes. In addition to GPS data, sport watches nowadays also record altitude (e.g. based on air pressure),heart rate or further attributes. Through this collected data, own behavior patterns can be predicted, such as fitness progress, stress, etc. In this work, it will be evaluated whether existing tracks of an athlete can be used to predict the individual time for a given route. This service is already available on different online-platforms such as *googlemaps* or *SchweizMobil*, but neither the fitness level, age, weight, nor the gender are taken into account for those calculations. This is the reason why apps that can evaluate individual data bring an advantage here. However, to enable this interpretation step from own collected data, there is a need of a) evaluating the accuracy of the measured data and b) determining the variables that lead to the desired output (in this case a time estimation). In the study by @gilgen-ammann2020a, the accuracy of the recorded distances of eight commercially available sports watches from Apple, Coros, Garmin, Polar and Suunto was examined in different areas and at different speeds. The results showed that the recorded systematic errors (±limits of agreements) when all measurement areas combined ranged between 3.7 (±195.6) m and -101.0 (±231.3) m, and the mean absolute percentage errors ranged from 3.2% to 6.1%. These results lead to the conclusion that the acquired coordinates should be aligned with a suitable digital elevation model DEM (e.g. SwissALTI3D) before further use of the data.

## Research Questions

The present study focuses on the following research questions:

1.  What are the most relevant influencing variables on the athlete's time requirement and speed in the analyzed data?

2.  What behavior patterns (e.g. pausing, moving) can be identified from the data? Are these behavior patterns related to distance and altitude differences? Can patterns in behavior be identified that are harmful to health? (Comparison of heart rate curve with recommended maximum heart rate).

3.  How reliably can the time required for sporting activities be predicted based on a user's individual fitness level? A model is created from past GPS tracks, which is validated using a self-experiment (determine new route using *SchweizMobil*, model time requirement and check the model output in a self-experiment).

# Methods

In the following subchapters the methodology is described in detail. The data sources and its import in an R-environment, the data processing and evaluation as well as the method for the creation of a time model are shown.

Since this report contain different Shiny apps, the HTML will only run if the Markdown is open in the background. When opening the HTML report only the most relevant outputs are shown in order to confine to the essential results. When using the provided R-Markdown (.Rmd), the underlying R code can be followed in the corresponding method and result chapters.

## Data sources and import

The data of sport tracks which can be obtained via the Garmin portal comes within the *.fit* format. The raw *.fit* file include spatio-temporal information, such as timestamp, position (Latitude and Longitude) as well as other variables like heart-rates, distance, enhanced-speed, enhanced-altitude (air pressure based) and cadence (cadence is defined as the total number of steps per minute).

A dataset (data.frame) was generated using sports tracks data from Garmin sportwatches from two different anonymous athletes (athlete 1, athlete 2). For this purpose, the files of every athlete must be labeled with *\_a1* or *\_a2* right before the file extension *.fit* and saved in the project directory in advance. Thus, files from athlete 1 always are labeled with *\_a1.fit* at the end of the file name. An example of a track by athlete 1 would be: *"6128842359_ACTIVITY_a1.fit".* Tracks of athlete 2 are for example labeled like *6128842359_ACTIVITY_a2.fit".* This allows adding additional data from other athletes anytime.

A for-loop detects all files from athlete 1 and athlete 2 in the project directory. All files per athlete will be combined into one dataset (data.frame) and labeled with a unique athlete ID. For the further steps, the user must specify which athlete is to be considered, because only one athlete can be analyzed at the same time. This choice can be made at the request *"Pick Athlete"* in the R-Code*.* line 141. The following sections of methodology and results refer to the evaluation of athlete 1's tracks. The discussion also identifies differences from athlete 2 and assesses whether the methodology presented here could be suitable for evaluating other individuals.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
############### Filter data fro one athlete only 
# Files of every athlete must be labeled with _a1, _a2, a_3 or a_x before the .fit extension. (x for any further athlete)

#  Get all the files per athlete
myfiles_a1 <- list.files(".",pattern = "*a1.fit")
myfiles_a2 <- list.files(".",pattern = "*a2.fit")

# Import files from athlete 1 temporary
for (i in 1:length(myfiles_a1)) {
  varName <- paste0("temp_a1", i, ".fit")
  assign(varName, as.data.frame(records(readFitFile(myfiles_a1[i]))))
}

# Bind row for athlete 1 temporary, label as athlete 1
a1 <- mget(ls(pattern="temp_a1")) %>%
              bind_rows() %>%
  mutate(athlete = 1)

# Import files from athlete 2 temporary
for (i in 1:length(myfiles_a2)) {
  varName <- paste0("temp_a2", i, ".fit")
  assign(varName, as.data.frame(records(readFitFile(myfiles_a2[i]))))
}

# Bind rows from athlete 2 temporary, label as athelte 2
a2 <- mget(ls(pattern="temp_a2")) %>%
              bind_rows() %>%
  mutate(athlete = 2)

# Bind files from all athletes using rowbind.
full <- bind_rows(a1, a2)

keep(full, sure = TRUE)


# Filter the whole data according to athlete to be analyzed: Pick an athlete!
Pick_Athlete <- 1

full <- full %>%
  filter(athlete == Pick_Athlete)

############# Filter here! User defines which athlete is considered (andd)

#check full
head(full)
```

## Preparation and evaluation of data

In order to distinguish between the different tracks run by an athlete, an individual ID per track had to be created first. This was fulfilled by determining the time lag first, i.e. the time that elapsed between 2 recorded points. A new "activity_ID" was then created if timelag \> 1 hour (3600s).

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Calculate timelag
full$timelag <- as.integer(difftime(full$timestamp, lag(full$timestamp)))
# activity_ID (new ID if timelag > 1 hour)

full$activity_conter <- ifelse(abs(full$timelag) > 3600, TRUE, FALSE) 
full$activity_conter[1] <- TRUE  # ID 1 = TRUE, as this is activity one

full$activity_ID <- cumsum(full$activity_conter == TRUE)  # create acitiviy_ID based on counter
```

Since time steps between individual activities are not relevant, the timelag is subsequently overwritten by only calculating timelags within a group with the same activity_ID. This excludes times between activities as these values are set to NA (not available).

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
full <- full %>%
  dplyr::group_by(activity_ID) %>%
  mutate(timelag = as.integer(difftime(timestamp, lag(timestamp))))
summary(full$timelag)
```

Some variables like distance, speed ("enhanced_speed") and altitude ("enhanced_altitude") are already available in the .fit files, nevertheless there is a need of comparison with the self calculated results in order to verify the accuracy. The methodology used for this is described in more detail in the following sub-chapters.

### Compare elevation by pressure with DEM

As the GPS data arrived from the sportwatch comes in the wrong coordinate system, an sf file was created ("full_sf") and convert into the Swiss Swiss national coordinate system *CH1903+ LV95*.

To evaluate the altitude data of the device derived from air pressure, a suitable digital elevation model (DEM) was taken and used in order to compare the information of the *.fit* files. The used DEM is available as Cloud Optimized GeoTIFF from *Swisstopo* and is called *SwissALTI3D*. A low raster size of 2m was used to achieve high quality information. The DEM is divided into different GeoTIFF files, each covering an area of one square kilometer. Reading and merging all DEM Files from Switzerland or the Canton of St.Gallen (Since athlete 1 has only run in the canton of St.Gallen) would be very time and resource consuming. Therefore, the area of investigation was defined automatically on the basis of the tracks read in and then only the necessary DEM raster files were imported.

The URLs to the files consist of a fixed part and a part dependent on the location. This dependent part was determined on the basis of the examination area. For this purpose, each x- and y-coordinate was floor rounded to one kilometer. This corresponds to the dependent part of the URLs. Then, all unique combinations of rounded x- and y-coordinates were then listed and the necessary URLs defined by the unique combinations and the fixed part of the links. As the URLs contain the date of the latest reevaluation of the DEM, some links contain the number 2019 while some others contain the number 2020. Thus, all the links were created twice (each for 2019 and 2020). In comparison to the complete list of all available DEM files in Switzerland, the correct URLs (containing the year 2019 or 2020) were selected and used for the import of the DEM rasters. For this import, a for loop was programmed, which downloads the required files from the URLs described before and merges them into one single raster.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Create sf and convert to CH1903+ LV95
full_sf <- st_as_sf(full, coords = c("position_long", "position_lat"),
         crs = 4326)
full_sf <- st_transform(full_sf, crs = 2056)

options(digits = 3)

# get x and y coordinates from sf geometry
full_sf$x <- st_coordinates(full_sf$geometry)[,1]
full_sf$y <- st_coordinates(full_sf$geometry)[,2]

##  digital elevation model to verify altitude for each GPS location
#    we only import the necesary data now!

# Create a Dataframe containing all positions (floor-rounded to 1km) as information source for required DHM-rasters
DHM_rasters <- data.frame(
  x = floor(full_sf$x / 1000),
  y = floor(full_sf$y / 1000)
)
# There are NA's when GPS-Position wasn't found (Activity_ID = 19) => Exclude them!
DHM_rasters <- filter(DHM_rasters, x > 0)

# Only keep the unique combinations for rounded x and y
DHM_rasters <- unique(DHM_rasters[c("x", "y")])

# Create the Download-Links of the tiff-files from Swisstopo: 
#  Sometimes data is from 2020, sometimes from 2019. Links aren't the same. So every combination has to be produced for 2019 data and 2020 data.
DHM_URL <- c(
  paste("https://data.geo.admin.ch/ch.swisstopo.swissalti3d/swissalti3d_2019_", DHM_rasters$x, 
        "-", DHM_rasters$y, "/swissalti3d_2019_", DHM_rasters$x, "-", DHM_rasters$y, 
        "_2_2056_5728.tif", sep = ""),
  paste("https://data.geo.admin.ch/ch.swisstopo.swissalti3d/swissalti3d_2020_", DHM_rasters$x, 
        "-", DHM_rasters$y, "/swissalti3d_2020_", DHM_rasters$x, "-", DHM_rasters$y,
        "_2_2056_5728.tif", sep = ""))

# Create a data.frame for join later...
DHM_URL <- as.data.frame(DHM_URL)

# Read all available DEM-sources for whole Switzerland
DHM_full <- read_csv("DHM_CH.csv", col_names = "DHM_URL")

# Keep only DEM-sources, which are available in the source DHM_full (wohle Switzerland)
#  This ensures that the correct link (2019 or 2020) is chosen from DHM_full
DHM_required <- inner_join(DHM_URL, DHM_full)

# Now import all the required URL's as raster-list
r.list <- list()
for(i in 1:length(DHM_required$DHM_URL)){  
  r.list[[i]] <- raster(DHM_required$DHM_URL[i])  
} 
```

As shown in Figure 1 below, only the DEM-rasters of the running area have been imported automatically. The points additionally show all the x- and y- coordinates that have been tracked by the athlete among all activities. They are all covered by the imported DEM.

```{r echo=FALSE, fig.cap="Figure 1: Digital elevation model (DEM) of the ovservation area with x and y coordinates of all analysed tracks", message=FALSE, warning=FALSE}
# Create a Rasterlayer from raster-list
m <- do.call(merge, r.list)

# Visialize imported data
plot(m, xlab="Longitude", ylab="Latitude")  
# This should exactly cover the tracks used

points(full_sf$x, full_sf$y)   # Add all points to the DEM-Graphic


```

Finally, the height information from the DEM could be extracted for every single x- and y-coordinate of the tracks to be analyzed. By joining the height information from the DEM with the coordinates of the tracks the investigation about accuracy of the elevation measurement by the pressure became possible. In the results, the absolute heights of both, the DEM and the pressure measurement are shown, as well as the height difference between the two data sources. Since the pressure measurement leads to constant fluctuations in the altitude information (see chapter [3.1.1]{.ul}), the altitude from the DEM is considered more reliable and is used for subsequent evaluations.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Reading of other tracks all over Switzerland will result in the import of the required rasters above!! :)

# Extract elevation information from rasterlayer
data.matrix <- as.data.frame(rasterToPoints(m))

# Create identical names for join in DEM source:
names(data.matrix)[names(data.matrix) == "x"] <- "x_round"
names(data.matrix)[names(data.matrix) == "y"] <- "y_round"

# create rounded x and y as join-key => every two meters, always odd values!
full_sf$x_round <- as.numeric(2 * round(full_sf$x/2) + 1)  
full_sf$y_round <- as.numeric(2 * round(full_sf$y/2) + 1)


# Join elevation 
full_sf <- left_join(full_sf, data.matrix, by = c("x_round", "y_round"))

#####  dplyr::rename  => change layer to altitude_DEM (andd)

# Calculate altitude difference between pressure measurement and DHM elevation (layer):
full_sf$altitude_diff <- full_sf$layer - full_sf$enhanced_altitude
```

### Compare recorded distance with Euclidean Steplength

Not only the altitude but also the distance recorded by the watch was compared with own calculations. Therefore, the Euclidean step length was calculated with data from the prepared data.frame "full_sf" where the GPS tracks were converted into the Swiss national coordinate system *CH1903+ LV95* (See chapter 2.2.1). As a preparation, for each running track the whole distance (Track_Distance), Euclidean Steplength (steplength), the whole raced distance by a given timepoint (steplength_sum), highdifference calculated with data from DEM (highdiff) and highdifference up, if highdiff \>0 and down if highdiff \<0 were calculated.

The difference between the distance derived by the sportwatch and distance calculation by Euclidean distance was compared and presented later in the results.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# calculate Euclidean step length and speed
#steplength_sum is distance raced at specific timepoint for each track 
full_sf <- full_sf %>%
  group_by(activity_ID) %>%
  mutate(
    Track_Distance = max(distance),
    steplength = sqrt((x - lead(x))^2 + (y - lead(y))^2),
    steplength_sum = lag(cumsum(replace_na(steplength,0))),
    hightdiff = lead(layer) - layer, # laver is the elevation from the DEM!
    hightdiff_up = ifelse(hightdiff > 0, hightdiff, 0),
    hightdiff_down = ifelse(hightdiff < 0, hightdiff, 0)
  )

#compare distance and steplength_sum

#generate random tracks to check
random_tracks <-sample(full_sf$activity_ID, 6)

#### Distance (absolute or in percent) instead of comparison

full_sf$distance_diff <- (full_sf$steplength_sum - full_sf$distance) / full_sf$Track_Distance
```

### Rolling windows

In this chapter a rolling window with different k-values was generated in order to smooth the time step-values. In figure 2 violin plots of all tracks from athlete 1 are shown. Further in figure 3 all smoothed speed curves are collected and compared to each other. Enhanced_speed_00 is the curve without rolling window (enhanced_speed multiplied by 3.6 to get the unit km/h). Since low values (around 0 km/h) are important to pick out the breaks in further steps, it must be ensured that these values are not be lost through smoothing. For this reason k=3 was defined and used for further calculations.

```{r message=TRUE, warning=FALSE, include=FALSE, paged.print=TRUE}

# speed from tracker in km/h
full_sf$enhanced_speed_kmh <- full$enhanced_speed *3.6

# smooth speed and heart rate by rollmenans

full_sf <- full_sf %>%
  group_by(activity_ID, athlete) %>%
  mutate(
    enhanced_speed_00 = enhanced_speed_kmh,
    enhanced_speed_02 = rollmean(enhanced_speed_kmh, k = 2, fill = NA, allign = "left"),
    enhanced_speed_03 = rollmean(enhanced_speed_kmh, k = 3, fill = NA, allign = "left"),
    enhanced_speed_05 = rollmean(enhanced_speed_kmh, k = 5, fill = NA, allign = "left"),
    enhanced_speed_10 = rollmean(enhanced_speed_kmh, k = 10, fill = NA, allign = "left"),
    enhanced_speed_50 = rollmean(enhanced_speed_kmh, k = 50, fill = NA, allign = "left")
  )

full_sf <- full_sf %>%
  group_by(activity_ID) %>%
  mutate(Track_Distance = max(distance)) %>%
  ungroup()

#check moving windows for track distances > 10km
full_sf %>%
  filter(Track_Distance > 10000) %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

#check moving windows for track distances < 10km
full_sf %>%
  filter(Track_Distance < 10000) %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

#generate one random track in order to check (for athlete 1)
random_track <-sample(full$activity_ID, 1)
```

```{r echo=FALSE, message=TRUE, warning=FALSE, paged.print=TRUE, fig.cap="Figure 2: Frequency of speeds in km/h depending on rolling window size"}
######## Use this graph for report
full_sf %>%
  gather(window, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(window, speed)) +
  geom_violin() +
  facet_wrap(~as.Date(timestamp))+
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "\nsize of the moving window", y = "speed in km/h\n")

```

```{r echo=FALSE, message=TRUE, warning=FALSE, paged.print=TRUE,fig.cap="Figure 3: Speed in km/h of a random track depending on rolling window size"}

speeds_rW <- full_sf %>%
  filter(activity_ID == random_track) %>%
  gather(smoother, speed, enhanced_speed_00 : enhanced_speed_50) %>%
  ggplot(aes(timestamp, speed, col = smoother)) +
  geom_line() +
  theme_bw()
ggplotly(speeds_rW)
#### USe k = 3 for further analysis
```

### Define static and moving points

This chapter is about distinguishing the pauses from the running parts in the different tracks. Since a break dosent mean that the speed is 0km/h (because its possible to walk around during a break), a threshold value to be defined. Therefore a histogram with all speed values from Athlete 1 was generated (see figure 4). From visual inspection of the histogram and together with the data from the violin plots from chapter 2.2.3, the threshold was defined at \<= 3.25 km/h. Therefore all speed values \<=3.25km/h were labeld as static = TRUE.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 4: Speed in km/h of a random track depending on rolling window size"}


#Remove “static points”

#Define threshold 

#plot trajectories with behavior pattern (static/moving) for Athlete 01 
#a) check data: histogram for speed for different tracks 


speed_mean <- mean(full_sf$enhanced_speed_03, na.rm=TRUE)

full_sf %>% 
  ggplot()+
  geom_histogram(aes(enhanced_speed_03), binwidth = 0.5)+
  geom_vline(aes(xintercept=speed_mean),linetype="dashed", color="blue")+
  geom_vline(aes(xintercept=3.25),linetype="dashed", color="red")+
  geom_text(x=3.25, y=-30, label="threshold", color="red")+#set threshold = 3.25 from visual context
  geom_text(x=speed_mean, y=-30, label="mean", color="blue")+
  theme_bw() +
  labs(x = "\nspeed in km/h (smoothed by rolling window k = 3)", y = "frequency\n")

### Define as static when speed < 3.25 km/h


##all points which are < than the threshold value of 3m/s of enhanced_speed_kmh are defined as static (moving)
#### Save in full_sf instead of tracks
full_sf <- full_sf %>% 
  ungroup() %>%
  mutate(static = enhanced_speed_03 <= 3.25)
```

### Define outliers

Since the goal for Research Question 3 is a generation of a time prediction model, the time system boundaries must be defined. Therefore, the time prediction must be in the range of the existing data. For this, the real running time was calculated first (from static = False) and the maximum race time was read out. For the model 10% to the maximum value from the existing is defined to be allowed. In the case of athlete 1 this means that the time prediction must not be longer than 3h.

To further detect outliers of the existing data / tracks an is_outlier function was created that will return a boolean TRUE/FALSE. Therefore, an often used rule was applied, which says that a value is an outlier if a value is more than 1.5 x IQR (interquartile range) above the upper quartile (Q3) or below the lower quartile (Q1). In other words, lower outliers are below (Q1 - 1.5 x IQR) and upper outliers are above (Q3 + 1.5 x IQR).

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#calculate duration of each track and
#calculate duration of each track without breaks and
#calculate duration of breaks per track

full_sf_filtered <- full_sf %>% 
  group_by(activity_ID) %>% 
  mutate(duration_whole_track = as.numeric(difftime(max(timestamp), min(timestamp), units = "hours"))) %>% #in hours
  ungroup() %>% 
  group_by(activity_ID, static) %>% 
  filter(static == FALSE) %>% 
    mutate(duration_moving = sum(timelag, na.rm = TRUE) / 3600) %>% 
  ungroup() %>% 
  mutate(duration_breaks = (duration_whole_track - duration_moving), 
         duration_breaks_min = (duration_breaks*60))


full_short <- full_sf_filtered %>% 
  group_by(activity_ID) %>% 
  summarize(duration_track = mean(duration_whole_track), 
            duration_moving = mean(duration_moving), 
            duration_breaks = mean(duration_breaks), 
            duration_breaks_min= mean(duration_breaks_min)) %>%
  ungroup() 

  ggplot(full_short)+
    geom_col(aes(x=(reorder(activity_ID,-duration_moving)),y=duration_moving))+
    coord_flip()+
    theme_bw()+
    ylim(0,3)+
    labs(title="moving duration of the tracks",
        x ="activity ID", y = "time in hours")
  
   
min(full_short$duration_moving)
max(full_short$duration_moving)

max(full_short$duration_moving)*1.1 #consider 10% deviation --> 3.01h 

  
#tracks duration beween 0.235 h and 2.77h for athlete 1  --> model prediction should not consider routes >3h

#investigation for outliers

#function for outlier hat will return a boolean TRUE/FALSE if the value passed to it is an outlier: 
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}
```

## Model individual time requirement

To model the time requirement of an activity, distance and altitude differences (upwards and downwards) were used as predictors in a first step. The distance derived by the sportwatch was used as a comparison with the Euclidean distance did not show any relevant differences. In contrast the altitude differences were calculated based on the DEM as these method seems to produce more accurate results. Subsequently, the model was refined by including fatigue (based on the time estimation of the first model). Fatigue was included because the first model led to a systematic overestimation of the time needed for short activities, whereas the time needed for longer activities was typically underestimated.

1.  Simple Model: Before starting with a multimodel inference (multiple regession), the activities were segmented in increments of one minute. Using multimodel inference, all predictors were checked for their importance. Subsequently, the model with the highest AICc value (lowest delta AICc respectively) was selected to model the individual time requirement of athletes.

2.  Exhaustion Model: Due to exhaustion, the simple model leads to an overestimation of the time requirement for short activities and tends to underestimate it for longer activities. For this reason, the "fatigue component" is added to the model. For this purpose, the time difference between prediction and measurement of each track is used and calculated using a linear model, based on the duration of the prediction. This results in a lower time requirement for short activities and a higher time requirement for longer activities. The slope of the linear model increases due to this correction, but by reducing the intercept, the time requirement decreases for short activities.

# Results

## Comparison data from sportwacht and calculated values

### Altitude by pressure vs. altitude from DEM

As described in the methods the accuracy of the pressure measurement by sportwatches had to be evaluated. Therefore, the pressure derived altitude from the watch was compared to a high resolution digital elevation model (DEM). The analysis includes a comparison of the absolute altitude measurement in meters above sea level and the altitude difference between the two data sources.

In the case of absolute altitude, there are only a few major differences between the two data sources. For example, activity 9 shows a comparatively large deviation at the beginning, see figure 5.

```{r echo=FALSE, fig.cap="Figure 5: Comparison of elevation calculation based on DEM and measurement based on air pressure of the sportwatch", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
# Test accuracy of elevation measurement
full_sf %>%
  st_drop_geometry() %>%
  dplyr::select(activity_ID, sportwatch_pressure = enhanced_altitude,
         DEM = layer, timestamp) %>%
  gather(source, value, sportwatch_pressure, DEM) %>%
  ggplot(aes(timestamp, value, col = source)) +
  geom_line() +
  facet_wrap(~activity_ID, scales = "free", ncol = 3) +
  theme_bw()+
  labs(x = "\ntimestamp hh:mm", y = "altitude measurement m a.s.l.\n")+
  theme(legend.position = "bottom")

```

The height difference of both data sources shows more or less permanent fluctuations. It should be noted that a constant variation of the altitude in total leads to a greater altitude difference than realistically completed. These fluctuations are caused on the one hand by the inaccuracy of the pressure measurement of the sport watch and on the other hand by the inaccuracy of the GPS fixes, which in turn influence the altitude based on the DEM. Based on this data, it is not possible to assess with certainty which data source is the more reliable. However, it is assumed that the GPS fix of the watch is more accurate than the pressure measurement and that the data from the DEM are therefore more reliable. The deviations between the two data sources are shown in figure 6.

```{r echo=FALSE, fig.cap="Figure 6: Difference between of elevation calculation based on DEM and measurement based on air pressure of the sportwatch", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
full_sf %>%
  ggplot(aes(timestamp, altitude_diff, col = altitude_diff)) +
  geom_point() +
  facet_wrap(~activity_ID, scales = "free", ncol = 3) +
  theme_bw() +
  theme(legend.position = "bottom") +
  labs(x = "\ntimestamp hh:mm", y = "altitude difference (DEM - pressure measurement) in m\n")
```

### Tracked distance vs. calculated distance

The comparison between the recorded distances and those from Euclidean Distance shows that in relation to the whole track the values deviate only between approx. -1% and +1%. This was calculated as calculated distance - recorded distance/ full track distance and therefore in percentage. Therefore, the distances tracked by the sports watch are reliable enough and can be further used.

```{r echo=FALSE, fig.cap="Figure 7: histogram for distance_diff (calculated distance - recorded distance/ full track distance)", message=FALSE, warning=FALSE, paged.print=FALSE}
ggplot(full_sf, aes(distance_diff))+
  geom_histogram(binwidth = 0.001)+
  labs(x= "\ndifference between data from sportwatch and euclidian distance\ncalculation of every timestep among all tracks", y = "frequency\n") +
  theme_bw()+
  scale_x_continuous(labels = scales::percent)
```

## Research Question 1

What are the most relevant influencing variables on the athlete's time requirement and speed in the analyzed data?

### Visualization of the tracks on map

There are various packages in R that can be used to visualize location data. In the following, some selected tracks are shown. Those visualizations provide an overview about the analyzed raw data. We have restricted ourselves to common packages such as leaflet() and ggmaps().

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# ggplot without map => not included in report
library(tidyverse)
ggplot() +
  geom_point(data = full_sf, aes(x, y, col = enhanced_speed_kmh)) +
  facet_wrap(~activity_ID, scales = "free", ncol= 4) +
  theme_bw()
```

The Package leaflet offers an interactive option for visualizing activities on a map, which is particularly impressive visually. In the following figures 8 and 9, two selected activities are visualized using leaflet. By zooming in, the accuracy of the GPS fixes can be analysed visually.

For example, if the user zooms into a forested region (between the locations of Krummenau and Ebnat-Kappel) in figure 6, it shows that there is sometimes a large deviation between the GPS fixes and the paths.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 8: Visualization of a track on a map using package leaflet"}

library(leaflet)

# Example track 1
m1 <- full %>%
  ungroup() %>%
  filter(activity_ID ==1) %>%
  dplyr::select(position_long, position_lat) %>% 
  as.matrix() %>%
  leaflet(  ) %>%
  addTiles() %>%
  addPolylines( )
m1
```

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 9: Visualization of a track on a map using package leaflet"}

# Example track 6
m2 <- full %>%
  ungroup() %>%
  filter(activity_ID ==3) %>%
  dplyr::select(position_long, position_lat) %>% 
  as.matrix() %>%
  leaflet(  ) %>%
  addTiles() %>%
  addPolylines( )
m2
```

Another possibility to visualize the activities on a map is offered by the package ggmap. It is based on the ggplot package, but allows the inclusion of map material. Due to the functionality of ggplot, speeds, for example, can be included in the visualization relatively easily by choosing color scales. The following figures 10 and 11 show the speed of the athlete as well as the altitude from one random track. When comparing the two graphs, it can be seen that the speed in areas with increasing altitude is lower than in the remaining areas. This shows the relevance of the altitude difference for the development of a model to predict the time required for activities.



```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 10: Visualization of a track on a map using package ggmap. The color scale represents the speed of the athlete. Figure 11: Visualization of a track on a map using package ggmap. The color scale represents the elevation at the coordinate."}

# Use ggmap for visualization
track1_3 <- full %>%
  filter(activity_ID == 1) %>%
  mutate(speed = enhanced_speed *3.6)

library(ggmap)

myLocation <- c(min(track1_3$position_long), min(track1_3$position_lat), 
                max(track1_3$position_long), max(track1_3$position_lat))  # Position definieren für Karten

myMap <- get_stamenmap(bbox=myLocation, maptype="terrain", crop=TRUE, zoom = 13)  # Kartenimport

map1 <-ggmap(myMap, size=c(640, 640)) +
  geom_point(data = filter(track1_3, speed > 3.25), 
             aes(position_long,position_lat, col = speed)) +
  scale_color_gradient(low = "yellow", high = "red")

track1_3 <- full %>%
  filter(activity_ID == 1) %>%
  mutate(speed = enhanced_speed *3.6)



myLocation <- c(min(track1_3$position_long), min(track1_3$position_lat), 
                max(track1_3$position_long), max(track1_3$position_lat))  # Position definieren für Karten

myMap <- get_stamenmap(bbox=myLocation, maptype="terrain", crop=TRUE, zoom = 13)  # Kartenimport

map2 <-ggmap(myMap, size=c(640, 640)) +
  geom_point(data = filter(track1_3, speed > 3.25), 
             aes(position_long,position_lat, col = enhanced_altitude)) +
  scale_color_gradient(low = "yellow", high = "red")


ggarrange(map1, map2, 
          ncol = 2, nrow = 1, 
          widths = c(1, 1), 
          heights = c(1,1))
```

### Variables affecting the time requirement

To answer research question 1, an exploratory data analysis (EDA) of the collected tracks was first conducted.

For a first inspection, a Shiny app (see figure 12) was programmed to browse through the different tracks of one athlete and compare the relationship of the speed, the altitude meters and the heart rate. Via dropdown menu it is possible to choose between the different collected tracks:

```{r echo=FALSE, fig.cap="Figure 12 Shiny app which show graphically how the variables speed, altitude difference and heart rate behave during different collected tracks from one athlete.", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
full_long <- gather(full_sf, type, value, layer, heart_rate, enhanced_speed_03)


library(shiny)
library(plotly)

#select plots which show graphically how the variables speed, altitude difference and heart rate behave.

#create function for generation of plots

plot_speed_altitude_heartrate_function <- function(track_ID){
  
  full_long %>%
  filter(activity_ID == track_ID) %>%
  ggplot(aes(timestamp, value, col = type)) +
  geom_line()+
  geom_point() +
  facet_wrap(~type, scales = "free",ncol = 1) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "\ntimestamp in hh:mm", y = "Value\n")
  
}

#count number of tracks (unique activity ID)
number_of_tracks <- length(unique(full_sf$activity_ID))

all_plots_speed_altitude_heartrate <- list() #create empty array

#save plots in array "all_plots_speed_altitude_heartrate"
for (track_ID in 1:number_of_tracks) {
    all_plots_speed_altitude_heartrate[[track_ID]]<-plot_speed_altitude_heartrate_function(track_ID)
  }
#create userinterface ui with shiny
ui <-shinyUI(fluidPage(selectInput("selectPlot", "Choose desired track", choices=1:number_of_tracks), plotlyOutput("plot")))

server <- shinyServer(function(input,output){      
  output$plot <- renderPlotly({
    all_plots_speed_altitude_heartrate[[strtoi(input$selectPlot)]]
  })
})

#get userinterface 

shinyApp(ui,server, options = list(height = 470))
```

Figure 12: Shiny app which show graphically how the variables speed, altitude difference and heart rate behave during different collected tracks from one athlete.

### Summary of tracks

A summary of all analyzed tracks is shown in table 1. This shows that activities in a distance range from x to y kilometers were used. The positive altitude difference per track were between x and y m, the negative altitude difference was measured between x and y m. It also shows the average pace (time required per kilometer of distance). The pace seems to be dependent on the distance and the altitude differences in the sense of an explorative data analysis.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
#### Use values from full_short! 
library(knitr)
zf <- full_sf_filtered %>% #zf because of "Zusammenfassung" in German
  st_drop_geometry() %>%
  group_by(activity_ID) %>%
  summarise(
    date = date(timestamp[1]),
    distance = max(distance, na.rm = TRUE),
    time = sum(timelag, na.rm = TRUE)/60,
    hightdiff_up = sum(hightdiff_up, na.rm = TRUE),
    hightdiff_down = sum(hightdiff_down, na.rm = TRUE),
    mean_pace = time / distance * 1000
    )
kable(zf[order(zf$mean_pace, decreasing = F),], caption = "Table 1: Summary of all tracks, ordered by descending mean pace")
```

## Research Question 2

In this chapter the goal was to indicate specific behavioral patterns (e.g. speed, breaks, or heart pattern) in the sports data and when they occur.

### Moving/breaks

In order to produce a model in chapter 3.5, some framework conditions must first be established. For this purpose, the length of the tracks, the actual running time and the breaks per track were identified.

From the statistical analysis (see chapter 2.2.4) we see that the running time for the prediction model should not be longer than 3h (system boundary).

The following boxplot (figure 13) shows that mainly tracks of 1 h were collected. For the model in Research Question 3, this means that the prediction works best for routes up to 1h. Longer routes (up to 3h) become inaccurate, because only few data are available. Track 1 and 19 are not excluded here as outliers, but it is pointed out that prediction for longer tracks (up to 3 hours) can become inaccurate.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 13: Identification of outliers"}
full_short %>%
  mutate(outlier = ifelse(is_outlier(duration_moving), activity_ID, as.numeric(NA))) %>%
  ggplot(aes(x = activity_ID, y = duration_moving)) +
    geom_boxplot() +
    geom_text(aes(x = 10.25, label = outlier), na.rm = TRUE, hjust = -0.2)+
  theme_bw()+
    labs(title="moving duration of the tracks",
        x ="", y = "time in hours\n") +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  theme_bw()
#check track 1 / 19 for possible outlier

```

In figure 14 we see another shiny app which allows to switch between different tracks in order to visualize running and breaking sections.

```{r echo=FALSE, fig.cap="Figure 14: Shiny app to select different tracks to examine the breaks in the traces ", fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
s1 <- data.matrix[seq(1, nrow(data.matrix), 100), ]  # Resize DHM raster to reduce the computing time per plot
s1 <- rename(s1, elevation = layer)
#create function for generation of plots for each track
plot_moving_breaks_function <- function(track_ID){
  
  full_sf%>% 
  filter(activity_ID == track_ID) %>%
  ggplot(aes(x = x, y = y))  +
  geom_path() +
  geom_raster(s1, mapping= aes(x_round, y_round, fill = elevation)) +
  geom_point(aes(colour = static)) +
  theme(legend.position = "right")+
  ggtitle("Moving/breaks during different tracks")+
  coord_fixed() +
  xlim (min(filter(full_sf, activity_ID == track_ID)$x - 350, na.rm = T),
        max(filter(full_sf, activity_ID == track_ID)$x + 350, na.rm = T)) +
  ylim (min(filter(full_sf, activity_ID == track_ID)$y - 550, na.rm = T),
        max(filter(full_sf, activity_ID == track_ID)$y + 550, na.rm = T)) +
  scale_fill_gradientn(colours = terrain.colors(7)) +
  coord_fixed() +
  theme_bw()+
  theme(legend.position = "bottom")
  
}

#count number of tracks (unique activity ID)
number_of_tracks <- length(unique(full_sf$activity_ID))

all_plots_moving_breaks <- list() #create empty array

#save plots in array "all_plots_moving_breaks"
for (track_ID in 1:number_of_tracks) {
    all_plots_moving_breaks[[track_ID]]<-plot_moving_breaks_function(track_ID)
  }

#create user interface ui with shiny
ui <-shinyUI(fluidPage(selectInput("selectPlot", "Choose desired track", choices=1:number_of_tracks), plotlyOutput("plot")))

server <- shinyServer(function(input,output){      
  output$plot <- renderPlotly({
    all_plots_moving_breaks[[strtoi(input$selectPlot)]]
  })
})

#get userinterface 
shinyApp(ui,server, options = list(height = 470))


```

Figure 14: Shiny app to select different tracks to examine the breaks in the traces.

Figure 15 shows the relationship between running time and break-time in minutes. The longer the running time, the longer the breaks required. Track 8 was excluded as it involves an extraordinary long pause (defined as outlier).

```{r  message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#Breaks in relation to the runtime

ggplot(full_short, aes(x=duration_breaks, y=duration_moving)) + 
  geom_point() +
  geom_text(aes(label=activity_ID),hjust=0, vjust=0)+
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))+
  labs(x = "\nduration of breaks [h]", y = "duration of moving [h]\n")+
  theme_bw()
```

```{r echo=FALSE, fig.cap="Figure 15: relationsip between running (in hours) and pause time (in minutes)", message=FALSE, warning=FALSE, paged.print=FALSE}
#excluding track 8 as it include an exeptional long break

full_short %>% 
  filter(!activity_ID==8) %>% 
ggplot(aes(x=duration_breaks_min, y=duration_moving)) + 
  geom_point() +
  geom_text(aes(label=activity_ID),hjust=0, vjust=0)+
  geom_smooth(method="lm", fill='lightblue', size=0.5, alpha=0.5) +
  theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face="bold"))+
  labs(x = "\nduration of breaks [min]", y = "duration of moving [h]\n")+
  theme_bw()

```

### Heart rate patterns

According to @diemaxi2010 the maximum heart rate (HRmax) is calculated as 220-age for running and 200-age for biking (rule of thumb). Also, women-specific calculations such as HFmax = 206 - (age x 0.88) (also rule of thumb) exist @gulati2010. Since only data from male athletes exist in this work, HRmax will be counted by the first formula (220- age). As the HRmax depend on the age, it is important that the data used is up to date, since the fitness level can change with the age.

Here, the date of birth has to be adjusted in the code manually (birth_athlete) for the selected athlete.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#max heart rate --literature athlete 1 
#date of athlete 1
birth_athelete <- as.Date('1991-11-16')
now <- Sys.time()

age_a1 <- time_length(difftime(now, birth_athelete), "years")

max_heart_rate_literature <- 220-age_a1
high_rate <- (max_heart_rate_literature*0.86)
paste("The indivudial is ", round(age_a1,0), " years old", " which leads to a maximum heart rate of ", round(max_heart_rate_literature,0), " BPM and a good training value for this indivudual would be around ", round(high_rate,0)," BPM", sep = "")

tracks_heart <- full_sf %>% 
  group_by(activity_ID) %>% 
    mutate(high = heart_rate > (high_rate), na.rm = TRUE)


#make plot with heart_rate normal (training) and high rate
tracks_heart %>%
  ggplot(aes(x, y, col=(high)))  +
  geom_path() +
  geom_point() +
  coord_fixed() +
  theme_bw()+
  theme(legend.position = "right")+
  facet_wrap(.~activity_ID)+
  scale_color_manual(values=c("#E69F00", "red"))
```

According to the American Heart Association ( @mayoclinic2021 ), the following target heart rate are recommended:

-   Moderate exercise intensity: 50% to about 70% of your maximum heart rate.

-   Vigorous exercise intensity: 70% to about 85% of your maximum heart rate.

For beginners the target heart rate will even be smaller. It is the athlete's choice which training level to select. However, it should not be higher than 85%. In following figure 16 the parts which include highrates \>85% are highlighted.

If red areas appear (see Figure 16), a reduction in speed is recommended.

```{r echo=FALSE, fig.cap="Figure 16: tracks with indicated high pulse sections (colored in red)", message=FALSE, warning=FALSE, paged.print=FALSE}

#create function for generation of plots

plot_heart_function <- function(track_ID){
  
 #make plot with heart_rate normal (training) and high rate
tracks_heart %>%
  filter(activity_ID == track_ID) %>%
  ggplot(aes(x, y))  +
  geom_path() +
  geom_raster(s1, mapping = aes(x_round, y_round, fill = elevation)) +
  geom_point(aes(col = high)) +
  coord_fixed() +
  xlim (min(filter(full_sf, activity_ID == track_ID)$x - 350, na.rm = T),
        max(filter(full_sf, activity_ID == track_ID)$x + 350, na.rm = T)) +
  ylim (min(filter(full_sf, activity_ID == track_ID)$y - 550, na.rm = T),
        max(filter(full_sf, activity_ID == track_ID)$y + 550, na.rm = T)) +
  scale_fill_gradientn(colours = terrain.colors(7)) +
  theme_bw()+
  theme(legend.position = "right")+
  scale_color_manual(values=c("green4", "red"))
  
}

#count number of tracks (unique activity ID)
number_of_tracks <- length(unique(full_sf$activity_ID))

all_plots_heartrate <- list() #create empty array

#save plots in array "all_plots_heartrate"
for (track_ID in 1:number_of_tracks) {
    all_plots_heartrate[[track_ID]]<-plot_heart_function (track_ID)
  }

#create userinterface ui with shiny
ui <-shinyUI(fluidPage(selectInput("selectPlot", "Choose desired track", choices=1:number_of_tracks), plotlyOutput("plot")))

server <- shinyServer(function(input,output){      
  output$plot <- renderPlotly({
    all_plots_heartrate [[strtoi(input$selectPlot)]]
  })
})

#get userinterface 
shinyApp(ui,server, options = list(height = 470))

```

Figure 16: tracks with indicated high pulse sections (colored in red)

## Research Question 3

In this chapter the results about the research question 3 are presented. First, a simple model for time estimation was build and the accuracy of the estimations was calculated. Then, exhaustion was taken into account by a model extension. Additionally, a new jogging route (not covered in the analyzed tracks) was chosen. Before absolving this route, the time requirement was estimated using the exhaustion model. Finally, the time estimation of this new route was compared to the measurement.

### Simple model

As described in the methods all activities were segmented in increments of one minute. For these increments the distance and the altitude differences (upwards and downwards) were calculated. These increments were then used to model the time requirement based on the distance and the height differences.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
full_sf_min <- full_sf_filtered %>%
  st_drop_geometry() %>%
  group_by(activity_ID) %>%
  mutate(Track_Distance = max(distance)) %>%
  group_by("time" = cut(timestamp, "1 min")) %>%
  summarise(
    distance = sum(steplength, na.rm = TRUE),
    Track_Distance = Track_Distance[1],
    hightdiff_up = sum(hightdiff_up, na.rm = TRUE),
    hightdiff_down = sum(-hightdiff_down, na.rm = TRUE),
    hightdiff_balance = hightdiff_up + hightdiff_down,
    time_run = sum(timelag, na.rm = TRUE),
    mean_pace = time_run / distance * 1000 / 60
  ) %>%
  mutate(vertical_direction = ifelse(hightdiff_balance > 2, "up",
                                      ifelse(hightdiff_balance < -2, "down",
                                             "horizontal")))

# Create a linear Model to calculate time by dist, hightdiff:
min.model <- lm(time_run/60 ~ 0 + distance + hightdiff_up + hightdiff_down, data=full_sf_min)
options(na.action="na.fail")
allmodels.min <- dredge(min.model)   # Create all possible models

avgmodel.min<-model.avg(get.models(dredge(min.model,rank="AICc"),subset=TRUE))
summary(avgmodel.min) # All predictors are highly significant!

avgmodel.min$coefficients

# Extract coefficients from the best Model [1]
distance_param_min <- allmodels.min$distance[1]
up_param_min <- allmodels.min$hightdiff_up[1]
down_param_min <- allmodels.min$hightdiff_down[1]
options(digits = 4)
```

All models of the multimodel inference are shown in table 2. The model on the top uses all degrees of freedom (4) which means that the distance as well as the heigth differences are used to model the time requirement. As the model on the top shows the lowest AICc value, this model is further used. Model parameters are as follows:

-   Distance affects the time estimation by `r distance_param_min` minutes per meter of distance which corresponds to `r distance_param_min * 1000` minutes per kilometer.
-   Positive height difference affects the time estimation by `r up_param_min` minutes per meter of height difference.
-   Negative height difference affect the time estimation by `r down_param_min` minutes per meter of height difference.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

kable(allmodels.min[,1:7], digits = c(4,4,4,0,0,0,0),caption = "Table 2: Models derived by multimodel inference. The model on the top uses all degrees of freedom (df) and shows the lowest AICc value corresponding to a delta AICc of 0 whereas the other models are less relevant (showing higher delta AICc values).")
```

The above described simple model without exhaustion predicts the time requirement for all activities with some deviation to the measured time requirement. As figure 17 shows, the time estimation meets the measured time requirement for some activities pretty well, whereas the time requirement especially for long activities (measured time requirement) are mostly underestimated. This shows the effect of the exhaustion.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 17: Time estimation of the simple model without exhaustion in comparison with measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement."}
 
# Estimate time requirement based on above created model parameters
zf$time_estimated_min <- zf$distance *distance_param_min + zf$hightdiff_up * up_param_min + zf$hightdiff_down * down_param_min

# Plot measured and estimated time: Model underestimates time requirement for long activities,
#  and overestimates for short activities (see following plots)
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_min, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, estimated) %>%
  ggplot(aes(reorder(activity_ID, -distance), value, col = type)) +
  geom_point() +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "time requirement in minutes\n")

```

In addition to the absolute time requirement in minutes, the relative difference between the simple model and the measurement is shown in figure 18 on a relative scale (deviation between model and measurement in percent). The upper part of the graph shows the measured distance per activity, the bottom shows the deviation of the time in percent. It is clearly demonstrated, that the simple model overestimates the time requirement for short activities (up to around 5 kilometers) and generally underestimates the time requirement for longer activities. As this graph does not take height differences into account, some of long-distance tracks are predicted pretty well. Still, there seems to be a systematical error in the model.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 18: Relative deviation between time estimation of the simple model without exhaustion and the measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement. The upper part shows the distance per track in meters, the bottom part shows the deviation."}

# Exhaustion can be seen here
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_min, distance, hightdiff_up, hightdiff_down) %>%
  mutate(deviation = estimated / measured -1) %>%
  ggplot(aes(reorder(activity_ID, -distance), deviation, fill = deviation)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_label(aes(label = paste(round(deviation,2)*100, "%", sep = "")), fill = "white")+
  geom_xsidepoint(aes(y = distance)) +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "relative under- and overestimation of time requirement // distance")
```

### Exhaustion model

To correct the systematical error of the above presented model, the effect of exhaustion was calculated using a linear model. First, the absolute time difference between estimation and measurement was calculated. Subsequently, this difference was modeled by a linear model using the estimated time as predictor of the exhaustion. This leads to the following results:

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Adjust Model from above with exhaustion: Calculate the time difference between measurement and base
#  model by the estimated time of the model
zf$time_diff <- - zf$time_estimated_min + zf$time   # Time estimated - time measured
adjust.model <- lm(time_diff ~ time_estimated_min, data = zf)
summary(adjust.model)  # Intercept is significant, time estimation too. R2 = 0.61.
adjust.model$coefficients

# Create exhaustion coefficient and intercept
exhausing_coeff <- as.numeric(adjust.model$coefficients[2])
exhausting_interc <- as.numeric(adjust.model$coefficients[1])

sum.ex <- summary(adjust.model)
options(digits = 2)
```

-   The intercept of the simple model is to be corrected by `r as.numeric(adjust.model$coefficients[1])` minutes. This will lead to a lower time estimation for tracks with a low estimation in time requirement

-   The slope of the simple model will be adjusted by an addition of `r as.numeric(adjust.model$coefficients[2])` minutes per minute of estimated time requirement.

The summary of this linear exhaustion model is presented in table 3. The r squared of the model is `r sum.ex$r.squared` which means that there is a strong relationship between the time difference and the estimated time of the activity. Still, this adjustment with exhaustion is a linear model with a limited scope of validity. Nevertheless, the exhaustion model seems to predict the time requirement for activities more accurate among most of the tracks. While deviations of the simple model were between -21 and +12%, these deviations (figure 19) were reduced to a maximum of -11 % and + 13%. Furthermore the differences no more seem to be systematical.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 19: Relative deviation between time estimation of the exhaustion model and the measured time requirement for all tracks of the athlete ordered by decreasing time requirement of the measurement. The upper part shows the distance per track in meters, the bottom part shows the deviation."}
# Include Exhaustion into model:
zf$time_estimated_full <- zf$distance *distance_param_min + zf$hightdiff_up * up_param_min + zf$hightdiff_down * down_param_min + zf$time_estimated_min * exhausing_coeff + exhausting_interc

zf %>% 
  dplyr::select(activity_ID, measured = time, estimated = time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  mutate(deviation = estimated / measured -1) %>%
  ggplot(aes(reorder(activity_ID, -distance), deviation, fill = deviation)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_label(aes(label = paste(round(deviation,2)*100, "%", sep = "")), fill = "white")+
  geom_xsidepoint(aes(y = distance)) +
  theme_bw() +
  labs(x = "\nactivity-ID", y = "relative under- and overestimation of time requirement // distance")

```

### Comparison of the models and the measurement

Figure 20 additionally shows the measured time requirement per track in descending order as well as the prediction of both models. The exhaustion model seems to predict the time requirement more precise for the most of the activities.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 20: Measured time requirement and both model estimations per track, ordered by descending distances."}
zf %>% 
  dplyr::select(activity_ID, measured = time, estimated_simple = time_estimated_min,  estimated_exhaustion= time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, estimated_exhaustion, estimated_simple) %>%
  ggplot(aes(as.factor(reorder(activity_ID, -distance)), value, fill = type)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7)+
  theme_bw() +
  labs(x = "\nactivity-ID", y = "time requirement in minutes\n")
```

```{r include=FALSE}
# These were just further analyses that werent included in the report...
zf %>%
  dplyr::select(time, simple = time_estimated_min, exhaustion = time_estimated_full) %>%
  gather(model, value, simple, exhaustion) %>%
  ggplot(aes(time, value, col = model)) +
  geom_abline(intercept = 0, slope = 1, col = "red") +
  xlim(0,175) +
  ylim(0,175) +
  geom_point()+
  theme_bw() +
  labs(x = "time measured", y = "time estimated")

# There ist still deviation but less systematical!
zf %>%
  dplyr::select(time, simple = time_estimated_min, exhaustion = time_estimated_full) %>%
  gather(model, value, simple, exhaustion) %>%
  ggplot(aes(time, value, col = model)) +
  geom_abline(intercept = 0, slope = 1, col = "black") +
  xlim(0,75) +
  ylim(0,75) +
  geom_point()+
  theme_bw() +
  labs(x = "time measured", y = "time estimated")
  

# Exhaustion Model represents measurement better than simple model without exhaustion.
zf %>% 
  dplyr::select(activity_ID, measured = time, simple = time_estimated_min, exhaustion = time_estimated_full, distance, hightdiff_up, hightdiff_down) %>%
  gather(type, value, measured, simple, exhaustion) %>%
  ggplot(aes(distance, value, col = type)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

### Validation of the Exhaustion Model

```{r include = FALSE}
Running_Dist <- 13540     # Distance of a route
Running_up <- 712         # Hightdifference up
Running_down <- 488       # Hightdifference down

valid_time_simple <- Running_Dist * distance_param_min + 
  Running_up * up_param_min + 
  Running_down * down_param_min 
valid_time_simple         # Gives time estimate with simple model

valid_time_exhaustion <- valid_time_simple + exhausting_interc + valid_time_simple * exhausing_coeff
valid_time_exhaustion     # Gives time estimate with exhaustion model

effective_time <- 93.2
options(digits = 0)
```

In order to validate the exhaustion model, athlete 1 defined a route which was not covered in the observation area of the analyzed tracks. This route can be specified as follows (according to *SchweizMobil*):

-   Distance of `r as.integer(Running_Dist)` meters in total

-   A total of `r Running_up` meters of positive height difference

-   A total of `r Running_down` meters of negative height difference

Based on these model inputs, the simple model calculates a time requirement of `r valid_time_simple` minutes, whereas the exhaustion model leads to a time estimation of `r valid_time_exhaustion` minutes. Athlete 1 took `r effective_time` minutes to complete the course in the test. Thus the prediction of the exhaustion model is much closer to the measurement than that of the simple model. Nevertheless, it must be noted, that this test with a sample size of only one track is definitely to little to to enable statements about the accuracy of the model.

### Visualization of Exhaustion Model

Assuming a range of validity of the model with distances between 3 and 32 kilometers and positive altitude differences between 0 and 1000 meters, the time required for activities can be classified on the basis of the model and displayed on a graph. Figure 21 shows such a graph, where the predicted time demand can be read out on the vertical z-axis under different conditions (distances and altitude differences). However, the extremely broad scope of this observation means that the time prediction can only serve as a rough basis for planning. Effective times will deviate, among other things, due to the non-linearity of fatigue and environmental parameters. These aspects will be examined in more detail in the discussion.

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Figure 21: 3D-Model of the time estimation by the exhaustion model depending of various distances and hight differences. Hight differences are always calculated for both, upwards and downwards."}
# Show the time estimation of different distances and hightdifferences as a 3D plot
timetable <- data.frame(
  Distance_m = rep(seq(3000 , 32000,  500), each = 11),
  hight_difference_m = rep(seq(0,1000, 100), 59)
)

timetable$simple_model_min <-timetable$Distance_m *distance_param_min + 
  timetable$hight_difference_m * up_param_min + timetable$hight_difference_m * down_param_min

timetable$exhaustion_model_min <- round(timetable$simple_model + exhausting_interc + exhausing_coeff * timetable$simple_model,0)

timeplot <- plot_ly(timetable, x = ~Distance_m, y = ~hight_difference_m, z = ~exhaustion_model_min,
                    color = ~exhaustion_model_min)
timeplot
```

# Discussion

In the following subchapters, the accuracy of the data and the model are discussed. Additionally, the limits of the model to predict the individual time requirement for an athlete at a given route are commented. This leads to the final conclusion about existing problems in the prediction of the time requirement for sporting activities as well as topics to be studied further.

## Accuracy of the Data

The analyses showed a relatively high accuracy of the data in many areas. For example, the distance calculated directly by the sports watch as well as the speed agree very precisely with the calculation via the euclidean distance, with a maximum deviation of 1%.

In the area of altitude differences, there were larger differences between the measurement from the watch, which is based on air pressure, and the DEM. For this reason, the height difference based on the GPS-fixes and a DEM with high resolution was used for the modeling of the time demand. This approach could lead to problems in a larger study area, as the amount of data to be imported increases proportionally with the extension of the study area. Furthermore, the method developed here for importing the DEM is limited to Switzerland, as the underlying elevation model is only available for Switzerland from the source used. Alternative elevation models are available, but would require some additional programming effort.

Finally, however, all relevant parameters depend on the accuracy of the GPS fixes. Figure ?? shows that the deviations between the GPS fixes and the paths can sometimes be large. A visual analysis of tracks on the map revealed large differences, especially in areas with forest cover. These differences influence all predictors of the model (distance directly and altitude differences indirectly via the DEM).

## Limits and Accuracy of the Model

In addition to the imprecision of the data, the accuracy of the model and its scope must be questioned. After creating a simple model without fatigue, an extended model was created that can at least partially represent athlete fatigue. This improved the predictions, especially for very short and very long activities. Nevertheless, there are still fluctuations in a range of maximum 15% compared to the effective measured values.

The fatigue model is also a linear model that can only reflect the athlete's situation within a certain range of validity. A validation of the fatigue model with an additional activity showed a high accuracy of the prediction, but does not allow a conclusive statement about the quality of the model. The authors assume that the model can predict the time required for activities in a distance range of 5 to 20 km and an altitude difference of 200 to 800 m relatively accurately.

This automatically means that only time-prediction is recommended for routes between 0-3 h. However, based on previous data, more accurate results are provided for tracks up to 1h. For longer routes, more data would have to be consulted.

Finally, it should be mentioned that the results described here refer to the evaluation of a single athlete. Data from a second person are available and were examined in a separate run. This separate run showed that the method can also be applied to other persons. This means that other model parameters are generated and therefore the time prediction also changes. The goal of a time prediction based on the individual fitness level could thus be fulfilled. The methodology used here can be transferred to other athletes as desired, although the .fit files used must be exchanged and some individual parameters such as data of birth in Reserach Question 2.

## Further studies

Factors affecting time requirements (and heart rate) include not only the route and its path conditions. Other factors such as the

-   air temperature / weather conditions

-   age

-   pre-existing conditions (such as cardiovascular disease, high cholesterol, diabetes, etc.)

-   medications

-   smoker/non-smoker

-   emotions

or similar can also influence the time needed for a certain distance. These factors were not considered in this work. Such factors could be included for another more complex model (no regression model), which would lead to even more accurate results.

# References
